# Model 

***ä¸‡ç‰©çš†å¯å­¦ä¹ ï¼ï¼ˆæ•°æ®å¤„ç†å‡½æ•°ã€è·ç¦»åº¦é‡å‡½æ•°ã€æŸå¤±å‡½æ•°ã€æ¨¡å‹æœ¬èº« ......ï¼‰***



## Quick Understanding of Deep Learning

-   Trainer: Us

-   Student: Machine (Computer)

-   **Plan before training (Need to be appropriate for the applicaion and easy to learn):** 

    a Function with Parameters to be Learnt & some Hyper-parameters (MANUALLY PROVIDED) => $Building\ the\ Model$

-   **Data for learning:** 

    a Training Set => $Train\ Time$

-   **Notes summarizing the learning:** 

    the Function with learned Parameters => $Learnt\ Model$

    ==To come up with the right Function is the key stage of Deep Learning!!!==

-   **Data for checking learning effect (Homework):**  

    a Validation Set => $Validation\ Time$

-   **Errors Recording:**

    Loss Function => *We tell our machine how bad the model is with this (Usually include <u>penalty \<Regularization Loss></u> in case of over-fitting T-T; Encourage simpler model so it needs to overcome the Regularization to attain more complex model, which will be more robust than complex ones without penalty)* 

-   **Consolidate the knowledge:** 

    the Training-Validation Loop => $Optimization$  

-   **==Data for the FINAL EXAM:==** 

    ==a Testing Set unseen through any stages before => $Test\ Time$==



## Quick Understanding of Neural Networks

**Basic Form of a $N+1$ Layers NN Model:** 

-   ==A **`Layer`**==   

    -   **`Affine Net`** â¡ï¸

        >   ### Affine Transformation in Neural Networks
        >
        >   1.  **Linear Transformation**: This involves multiplying the input by a weight matrix. If you have an input vector $X$ and a weight matrix $W$, the linear transformation is $Xâ‹…W$.
        >
        >       ***=> scale the vectors***
        >
        >   2.  **Translation (Bias Addition)**: After the linear transformation, a bias vector is added to the result. If the bias vector is $b$, the full affine transformation is $Xâ‹…W+b$.
        >
        >       ***=> offset the vectors***
        >
        >   Affine Layers are usually **FC** Layers.

    -   *(OPTIONAL)* **`Batch Normalization`** or **`Layer Normalization`** â¡ï¸

    -   **`Non-Linear Activation Func`** (Like a firing rate of impulses carried away from cell body; most similar one to the actual brain is the **`ReLU`**, which is most commonly used) **=> works on non-linear features, bend the vectors (for larger distances and better classification)**â¡ï¸

    -   *(OPTIONAL)* **`Dropout`** â¡ï¸

-   **Stack " a `Layer`" $N$ times until it's a complex enough non-linear function (but need to trade-off with difficulty in tackling overfitting)** â¡ï¸

-   The **`Final Layer`**
    -   **`Affine Net`**â¡ï¸
    -   (OPTIONAL, <u>NEEDED IN CLASSIFICATION</u>) **`Softmax`**

<img src="images/image-20230819180311872.png" alt="image-20230819180311872" style="zoom: 25%;" />

![2e775eb44e3a0edeb4debd1f3a309cb](images/2e775eb44e3a0edeb4debd1f3a309cb.jpg)

ğŸ”¼***W åˆ—æ•°æ˜¯è¾“å…¥ç»´åº¦ï¼ŒW è¡Œæ•°æ˜¯è¾“å‡ºç»´åº¦ï¼ˆç¥ç»å…ƒä¸ªæ•°ï¼‰*** 

![image-20230819181131837](images/image-20230819181131837.png)

<img src="images/image-20230819181020242.png" alt="image-20230819181020242" style="zoom:50%;" />



## Quick understanding of Neurons and Layers

**â€œç¥ç»å…ƒâ€ = æƒé‡çŸ©é˜µ&åç§»é‡ï¼ˆ`nn.Linear`ï¼Œæƒé‡æ•°ä¹˜ç‰¹å¾å€¼=>å‘é‡ä¼¸ç¼©ï¼Œåç§»é‡åŠ å‡ç‰¹å¾å€¼=>å‘é‡å¹³ç§»ï¼‰ + æ¿€æ´»å‡½æ•°ï¼ˆéçº¿æ€§å‡½æ•°ï¼Œæ”¹å˜ç‰¹å¾å€¼é—´çº¿æ€§å…³ç³»=>å‘é‡å¼¯æ›²ï¼‰**

æ¯ä¸ªç¥ç»å…ƒè´Ÿè´£æ¥æ”¶ $X$ è¾“å…¥çš„æ‰€æœ‰ç‰¹å¾å€¼ $x_i$ å¹¶å¤„ç†æˆå•ä¸ªç‰¹å¾å€¼è¾“å‡ºã€‚

éšè—å±‚ç¥ç»å…ƒä¸ªæ•°å°±æ˜¯éšè—å±‚æ•´ä½“è¾“å‡ºçš„ç»´åº¦æ•°ï¼Œæ‰€ä»¥æ¯ä¸ªéšè—å±‚éƒ½æ˜¯ä¸€ä¸ªæ”¹å˜åŸå§‹è¾“å…¥ $X$ çš„ç»´åº¦çš„æœºä¼šï¼Œå³èƒ½å¤Ÿå¯¹ $X$ åšä»¿å°„å˜æ¢ï¼ŒæŠŠå¯èƒ½åœ¨åŸæ¥ç»´åº¦ä¸Šä¸å…¶ä»–æ ·æœ¬çº¿æ€§ä¸å¯åˆ†çš„ $X$ å˜æ¢åˆ°ä¸å…¶ä»–æ ·æœ¬çº¿æ€§å¯åˆ†çš„ç»´åº¦ä¸Šã€‚

**å…·ä½“å®ç°ä¸­ï¼Œå¯ä»¥æŠŠå·ç§¯æ ¸ï¼ˆæƒé‡çŸ©é˜µï¼‰çš„ä¸ªæ•°çœ‹ä½œç¥ç»å…ƒä¸ªæ•°ï¼Œå·ç§¯å±‚è¾“å‡ºçš„ç‰¹å¾å›¾çš„é€šé“æ•°å³è¯¥å±‚çš„ç¥ç»å…ƒä¸ªæ•°ã€‚**

### 1 neuron per layer

åªèƒ½å¤„ç†è¿™ç§åˆ†ç±»ï¼š

![760b66247c4df1e98cb41ec363b3257](images/760b66247c4df1e98cb41ec363b3257.jpg)

é‡åˆ°è¿™ç§åˆ†ç±»é¡»å˜æ¢åˆ°äºŒç»´ï¼š

![0266d79ef043b47f35d3a8678572216](images/0266d79ef043b47f35d3a8678572216.jpg)

å˜æ¢åï¼š

![7d8afdbc0fbf801f0b8ce2d9abbfb7b](images/7d8afdbc0fbf801f0b8ce2d9abbfb7b.jpg)

### 2 neurons per layer

åªèƒ½å¤„ç†è¿™ç§åˆ†ç±»ï¼š

![609a9a12876b37e8fd707681af347de](images/609a9a12876b37e8fd707681af347de.jpg)

é‡åˆ°è¿™ç§åˆ†ç±»é¡»å˜æ¢åˆ°ä¸‰ç»´ï¼š
![98c6518628582a7a69760d7e05d49c1](images/98c6518628582a7a69760d7e05d49c1.jpg)

å˜æ¢åï¼š

![e7edb4ff9122a5ecb7cf41c3fb06b6f](images/e7edb4ff9122a5ecb7cf41c3fb06b6f.jpg)

æ›´é«˜ç»´æƒ…å†µåŒç†ã€‚

### More layers

å³ä¾¿æœ‰äº†å˜æ¢åˆ°é«˜ç»´çš„ç½‘ç»œå±‚ï¼Œä¹Ÿä¸ä¸€å®šèƒ½æˆåŠŸåˆ†ç±»ï¼Œå› ä¸ºæœ‰çš„è¶…å¹³é¢åˆ†ç•Œåœ¨å…¶æ‰€åœ¨é«˜ç»´ç©ºé—´å†…ä¹Ÿå¾ˆéš¾è¢«æ‰¾åˆ°ï¼Œå› æ­¤éœ€è¦å¢åŠ ç½‘ç»œå±‚è¿›ä¸€æ­¥åœ¨åŒç»´ç©ºé—´å†…å˜æ¢æˆ–å˜æ¢åˆ°æ›´é«˜ç»´ç©ºé—´ï¼Œä»¥æ­¤æ›´å¥½æ‹‰å¼€è¢«åˆ†ç±»çš„ä¸¤ç±»æ•°æ®ç‚¹ï¼ˆå‘é‡ï¼‰ä¹‹é—´çš„è·ç¦»ã€‚

æ­¤å¤–ï¼Œåˆ†å±‚å­¦ä¹ å¯ä»¥ç‹¬ç«‹è®¾è®¡ä¸åŒå±‚çš„ä¸åŒæ¶æ„ä»¥å­¦ä¹ ä¸åŒç‰¹å¾ï¼ŒåŒæ—¶è¶Šæ·±çš„å±‚å¯ä»¥å­¦ä¹ åˆ°è¶Šéš¾å­¦ä¹ ï¼ˆè¶ŠæŠ½è±¡ï¼‰çš„ç‰¹å¾ï¼Œä»¥æé«˜å­¦ä¹ èƒ½åŠ›ä¸è®­ç»ƒæ•ˆç‡ã€‚

ä»åå·®ã€æ–¹å·®è§’åº¦çœ‹ï¼Œå°±æ˜¯é€šè¿‡å¢åŠ ç½‘ç»œå±‚æ‹‰å¼€æ•°æ®ç‚¹é—´è·ï¼Œæ¥æ‰©å¤§ä¸­é—´å¯å¯»æ‰¾çœŸå®è¶…å¹³é¢åˆ†ç•Œçš„ç©ºé—´ï¼Œå³æ‰©å¤§å¯¹çœŸå®è¶…å¹³é¢åˆ†ç•Œçš„è¦†ç›–ï¼Œä»¥é™ä½ä¼˜åŒ–ç»“æœä¸çœŸå®è¶…å¹³é¢åˆ†ç•Œé—´çš„æ–¹å·®ã€é™ä½å¯»æ‰¾åˆ°çœŸå®è¶…å¹³é¢çš„éš¾åº¦ã€‚

![61bd22ff661aafa4a228822423f6091](images/61bd22ff661aafa4a228822423f6091.jpg)

è§£å†³æ·±å±‚ç½‘ç»œåå·®çš„é—®é¢˜ï¼ˆç½‘ç»œé€€åŒ–ï¼‰åŠæ¢¯åº¦æ¶ˆå¤±é—®é¢˜ä½¿ç”¨ ResNetï¼Œè¯¦è§ **<u>Evaluation (of Generalization --- the closeness to ground-truth) & Solutions</u>**



## Activation Function

1.  **Sigmoid**:

    -   **Function**: 
        $$
        f(x)=\frac{1}{1+e^{-\beta x}}
        $$
        
    -   **Gradient**: 
        $$
        f'(x)=\beta f(x)(1-f(x))
        $$
    
    -   **Problematic Region**: The gradient becomes close to zero for very large negative or positive values of $x$. In these regions, the sigmoid function saturates, meaning that it becomes very flat. This leads to vanishing gradients during backpropagation, which can slow down or halt training.
    
2.  **ReLU (Rectified Linear Unit)**:

    -   **Function**:
        $$
        f(x)=max(0,x)
        $$
        
    -   **Gradient**: 
        $$
        f'(x)=\{^{1\quad if\ x>0}_{0\quad if\ x\le0}
        $$
    
    -   **Problematic Region**: The gradient is exactly zero for negative inputs. This can lead to "dead neurons" where once a neuron gets a negative input, it always outputs zero, and its weights never get updated. This is known as the dying ReLU problem.
    
3.   **[SoftMax](https://blog.csdn.net/bitcarmanlee/article/details/82320853)**

     -   **Function:**
         $$
         f(x_j) = \frac{e^{x_j}}{\sum^{n}_{i=1} e^{x_i}}
         $$

     -   **Gradient: ** $EntropyLoss = -\ln y_j$
         $$
         EntropyLoss'(x) = y_i-1
         $$

4.   **Leaky ReLU**:

     -   **Function**: 
         $$
         f(x)=\{^{x\quad if\ x>0}_{\alpha x\quad if\ x\le0}
         $$
         
     -   **Gradient**: 
         $$
         f'(x)=\{^{1\quad if\ x>0}_{\alpha\quad if\ x\le0}
         $$
         
     -   **Problematic Region**: Leaky ReLU attempts to fix the dying ReLU problem by having a small positive gradient for negative inputs. This means that the gradient is never exactly zero, but if $\alpha$ is very small, the gradient can still be close to zero for negative inputs, potentially slowing down training.


***COMPARISON***:

-   **Sigmoid** has vanishing gradient problems for very large negative or positive inputs.
-   **ReLU** has zero gradient for negative inputs, leading to the dying ReLU problem.
-   **Leaky ReLU** attempts to mitigate the dying ReLU problem but can still have near-zero gradients for negative inputs if $Î±$ is very small.

5.   **Swish**ï¼ˆReLUçš„æ›¿ä»£ï¼‰:

     -   **Function**:
         $$
         Swish(x)=x\cdot Sigmoid(x)
         $$


     -   **éå•è°ƒæ€§**ï¼šSwishæ˜¯ä¸€ä¸ªå¹³æ»‘çš„ã€éå•è°ƒçš„å‡½æ•°ã€‚è¿™ä¸ä¼ ç»Ÿçš„ReLUå‡½æ•°ï¼ˆçº¿æ€§ä¸”å•è°ƒï¼‰å½¢æˆå¯¹æ¯”ã€‚


     -   **æœ‰ç•Œçš„è´Ÿå€¼**ï¼šä¸ReLUä¸åŒï¼ŒSwishå‡½æ•°åœ¨è´Ÿå€¼æ—¶ä¸æ˜¯å®Œå…¨ä¸ºé›¶ï¼Œå®ƒå…è®¸è´Ÿå€¼é€šè¿‡ï¼Œè¿™å¯èƒ½æœ‰åŠ©äºä¿æŒç¥ç»ç½‘ç»œä¸­æ›´å¤šçš„ä¿¡æ¯æµã€‚


     -   **è‡ªé€‚åº”**ï¼šé€šè¿‡å¼•å…¥ $\beta$ å‚æ•°ï¼ŒSwishå‡½æ•°å¯ä»¥åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è‡ªé€‚åº”åœ°è°ƒæ•´å…¶å½¢çŠ¶ï¼Œè¿™åœ¨æŸäº›æƒ…å†µä¸‹å¯èƒ½æœ‰åŠ©äºæå‡æ¨¡å‹æ€§èƒ½ã€‚


     -   å’ŒReLUç›¸æ¯”çš„ä¼˜åŠ¿ï¼š
    
         -   **æ€§èƒ½æå‡**ï¼šåœ¨ä¸€ç³»åˆ—çš„åŸºå‡†æµ‹è¯•å’Œä»»åŠ¡ä¸­ï¼ŒSwishå‡½æ•°å±•ç¤ºäº†ä¸ReLUç›¸æ¯”åœ¨æ·±åº¦ç½‘ç»œä¸­çš„æ€§èƒ½æå‡ï¼Œå°¤å…¶æ˜¯åœ¨æ·±åº¦è¾ƒå¤§çš„ç½‘ç»œç»“æ„ä¸­ã€‚
             -   **å¹³æ»‘æ¢¯åº¦**ï¼šSwishå‡½æ•°ç”±äºå…¶å¹³æ»‘æ€§è´¨ï¼Œå¯ä»¥æä¾›æ›´ç¨³å®šçš„æ¢¯åº¦æµï¼Œæœ‰åˆ©äºæ·±åº¦å­¦ä¹ æ¨¡å‹çš„è®­ç»ƒã€‚
             -   **çµæ´»æ€§**ï¼šSwishå‡½æ•°é€šè¿‡å‚æ•° $\beta$ æä¾›äº†é¢å¤–çš„çµæ´»æ€§ï¼Œè¿™ä¸€å‚æ•°å¯ä»¥æ ¹æ®ä»»åŠ¡éœ€æ±‚è¿›è¡Œè°ƒæ•´æˆ–é€šè¿‡å­¦ä¹ å¾—åˆ°ã€‚

6.   The **Hyperbolic Tangent (tanh)** is an activation function used in neural networks, including RNNs. It's defined as:

$$
tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}
$$
-   The function maps any real-valued number to the range âˆ’1,1âˆ’1,1. Here's what it looks like:

-   Graph of tanh(x)Graph of tanh(x)

-   The tanh function is zero-centered, meaning that negative inputs will be mapped strongly negative and zero inputs will be near zero in the output. This makes it easier for the model to learn from the backpropagated error and can result in faster training.

-   Here are some properties of the tanh activation function:
    1.  **Non-linear**: This allows the model to learn from the error and make adjustments, which is essential for learning complex patterns.
    2.  **Output range**: The output values are bound within the range âˆ’1âˆ’1 and 11, providing normalized outputs.
    3.  **Zero-centered**: This helps mitigate issues related to the gradients and speeds up the training process.

***COMPARISON:***

### ReLU (Rectified Linear Unit)

1.  **Computational Efficiency**: ReLU is computationally cheaper to calculate than tanh because it doesn't involve any exponential operations. This makes it faster to train large networks.
2.  **Sparsity**: ReLU activation leads to sparsity. When the output is zero, it's essentially ignoring that neuron, leading to a sparse representation. Sparsity is beneficial because it makes the network easier to optimize.
3.  **Non-vanishing Gradients**: ReLU doesn't suffer from the vanishing gradient problem for positive values, which makes it suitable for deep networks.

### tanh (Hyperbolic Tangent)

1.  **Zero-Centered**: Unlike ReLU, tanh is zero-centered, making it easier for the model to learn in some cases.
2.  **Output Range**: The output range of tanh is [âˆ’1,1][âˆ’1,1], which can be more desirable than [0,âˆ)[0,âˆ) for ReLU in certain applications like RNNs.
3.  **Vanishing Gradients**: tanh can suffer from vanishing gradients for very large or very small input values, which can slow down learning.

### Context-Specific Usage

-   **RNNs**: tanh is often used because the zero-centered nature of the function can be beneficial for maintaining the state over time steps.
-   **CNNs and Fully-Connected Networks**: ReLU is often preferred due to its computational efficiency and because CNNs often deal with larger and deeper architectures where vanishing gradients are less of a concern.



## Embedding Layers

***ç›®çš„ï¼šé€šè¿‡éæ ‡ç­¾çš„å½¢å¼è¾“å…¥æ¡ä»¶ï¼›æŠŠä¸åŒæ¡ä»¶çš„å½±å“è§£è€¦åˆ°è¾“å…¥æ•°æ®çš„ä¸åŒæ–¹é¢ï¼ˆæ¯”ç‡&åå·®ï¼‰ï¼Œæ–¹ä¾¿è°ƒè¯•æ¨¡å‹***

**æ–¹å¼ä¸€ï¼š**åˆ†åˆ«ä»¥scalarå’Œbiasçš„å½¢å¼æŠŠä¸¤ä¸ªæ¡ä»¶å¼ é‡åœ¨åŒä¸€å±‚åµŒå…¥æ•°æ®ï¼ˆä¸€èˆ¬åœ¨Normalizationåï¼‰

>   ä¾‹ï¼š
>   $$
>   AdaGN(h,y)=y_s\cdot GroupNorm(h)+y_b
>   $$
>   $y=[y_s,y_b]$ï¼Œ $y_s$ æ˜¯æ—¶é—´æ­¥åµŒå…¥å¼ é‡ï¼Œ $y_b$ æ˜¯ç±»åˆ«æ ‡ç­¾åµŒå…¥å¼ é‡ï¼Œ $h$ æ˜¯å‰ä¸€å±‚å·ç§¯å±‚çš„ç»“æœ

**æ–¹å¼äºŒï¼š**åœ¨ä¸åŒå±‚ä»¥scalarï¼ˆæˆ–biasï¼‰çš„å½¢å¼æŠŠå¤šä¸ªæ¡ä»¶å¼ é‡ï¼ˆä¸¤ä¸ªä»¥ä¸Šï¼‰ä¾æ¬¡åµŒå…¥æ•°æ®ï¼ˆä¸€èˆ¬åœ¨Normalizationåï¼‰



## Hyper-Parameters

### Common Ones

-   **Batch Size**

    >   *It is usually based on memory constraints (if any), or set to some value, e.g. 32, 64 or 128. We **use powers of 2 in practice** because many vectorized operation implementations work faster when their inputs are sized in powers of 2.*

-   **(Initial) Learning Rate (set in Optimizer)** 

    >   ***Effect of step size**. Choosing the step size (also called the learning rate) will become one of the most important (and most headache-inducing) hyperparameter settings in training a neural network. In our blindfolded hill-descent analogy, <u>we feel the hill below our feet sloping in some direction, but the step length we should take is uncertain</u>. If we shuffle our feet carefully we can expect to make consistent but very small progress (this corresponds to having a small step size). Conversely, we can choose to make a large, confident step in an attempt to descend faster, but this may not pay off. At some point taking a bigger step gives a higher loss as we â€œoverstepâ€.*

-   **Momentum (set in Optimizer)**

-   **Dropout (set in NN Module)**

-   **Weight-decay (set in Optimizer)**

-   **Leak slope  (set in NN Module)**

### How to Tune

#### General

Initialize a set of HP -> Train on training set with this set of HP -> Get the best learned Params set (the Model) of this set of HP -> Predict with validation set -> Get an Acc -> ... (<u>**CROSS -VALIDATION**</u> LOOP UNTIL THE ACC IS SATISFYING ENOUGH)

#### K-Fold Cross validationï¼ˆç»“åˆSGDå¯åœ¨å¤§æ•°æ®é›†ä¸‹ä½¿ç”¨ï¼‰

ç”¨äºè°ƒèŠ‚**è¶…å‚æ•°**çš„éªŒè¯æ–¹æ³•ï¼Œä¸æ˜¯æ­£å¼è®­ç»ƒï¼ˆæ˜¯â€œè€ƒå‰æ¨¡æ‹Ÿè€ƒâ€ï¼‰ï¼š

1.  ä¾‹å¦‚å…±æœ‰Nç»„hpï¼Œå¯¹åº”Nä¸ªæƒ³éªŒè¯çš„æ¨¡å‹

2.  ==å¯¹äºæ¯ä¸€ä¸ªæ¨¡å‹ï¼ˆæ¯ä¸€ç»„hpï¼‰== 

    1.  shuffleæ•´ä¸ªè®­ç»ƒé›†

    2.  åˆ’åˆ†æˆ K ç­‰ä»½ï¼ˆæœ€åä¸€ä»½å¯ä¸è¶³ä¸€ç­‰ä»½ï¼‰

    3.  è¿›è¡Œ K åœºæ¨¡æ‹Ÿè®­ç»ƒï¼Œæ¯è½®è®­ç»ƒï¼š

        1.  åœ¨ **K-1/K** çš„è®­ç»ƒé›†ä¸Šä»¥ m ä¸ªæ•°æ®ç‚¹ä¸ºä¸€ä¸ªbatchè®­ç»ƒ n ä¸ªepoch
        2.  åœ¨å‰©ä¸‹ **1/K** çš„è®­ç»ƒé›†ä¸ŠéªŒè¯ï¼ˆä¹Ÿåˆ’åˆ†ç­‰å¤§çš„batchï¼‰

        æ€»å…±ç›¸å½“äº **K\*n ä¸ªè®­ç»ƒepoch**

    4.  æ±‚ K æ¬¡**éªŒè¯çš„å¹³å‡æŸå¤±å€¼/ å‡†ç¡®ç‡ $\mu$**

3.  é€‰å– $\mu$ æœ€å°ï¼ˆæŸå¤±å€¼ï¼‰/ å¤§ï¼ˆå‡†ç¡®ç‡ï¼‰çš„ä¸€ä¸ªæ¨¡å‹ï¼ˆä¸€ç»„hpï¼‰ä½œä¸ºæ­£å¼è®­ç»ƒä½¿ç”¨çš„æ¨¡å‹

4.  **æ­£å¼è®­ç»ƒï¼šä½¿ç”¨å®Œæ•´è®­ç»ƒé›†è®­ç»ƒä¸€ä¸ªepochå¾—åˆ°æœ€ç»ˆæ¨¡å‹**ï¼ˆè®ºæ–‡ä¸­å±•ç¤ºçš„ï¼‰

#### è‡ªåŠ¨è°ƒå‚æŠ€æœ¯

https://www.zhihu.com/question/347358336

#### More Tips

>   ### Hyperparameter optimization
>
>   As weâ€™ve seen, training Neural Networks can involve many hyperparameter settings. The most common hyperparameters in context of Neural Networks include:
>
>   -   the initial learning rate
>   -   learning rate decay schedule (such as the decay constant)
>   -   regularization strength (L2 penalty, dropout strength)
>
>   But as we saw, there are many more relatively less sensitive hyperparameters, for example in per-parameter adaptive learning methods, the setting of momentum and its schedule, etc. In this section we describe some additional tips and tricks for performing the hyperparameter search:
>
>   **Implementation**. Larger Neural Networks typically require a long time to train, so performing hyperparameter search can take many days/weeks. It is important to keep this in mind since it influences the design of your code base. One particular design is to have a **worker** that continuously samples random hyperparameters and performs the optimization. During the training, the worker will keep track of the validation performance after every epoch, and writes a model checkpoint (together with miscellaneous training statistics such as the loss over time) to a file, preferably on a shared file system. It is useful to include the validation performance directly in the filename, so that it is simple to inspect and sort the progress. Then there is a second program which we will call a **master**, which launches or kills workers across a computing cluster, and may additionally inspect the checkpoints written by workers and plot their training statistics, etc.
>
>   **Prefer one validation fold to cross-validation**. In most cases a single validation set of respectable size substantially simplifies the code base, without the need for cross-validation with multiple folds. Youâ€™ll hear people say they â€œcross-validatedâ€ a parameter, but many times it is assumed that they still only used a single validation set.
>
>   **Hyperparameter ranges**. Search for hyperparameters on log scale. For example, a typical sampling of the learning rate would look as follows: `learning_rate = 10 ** uniform(-6, 1)`. That is, we are generating a random number from a uniform distribution, but then raising it to the power of 10. The same strategy should be used for the regularization strength. Intuitively, this is because learning rate and regularization strength have multiplicative effects on the training dynamics. For example, a fixed change of adding 0.01 to a learning rate has huge effects on the dynamics if the learning rate is 0.001, but nearly no effect if the learning rate when it is 10. This is because the learning rate multiplies the computed gradient in the update. Therefore, it is much more natural to consider a range of learning rate multiplied or divided by some value, than a range of learning rate added or subtracted to by some value. Some parameters (e.g. dropout) are instead usually searched in the original scale (e.g. `dropout = uniform(0,1)`).
>
>   **Prefer random search to grid search**. As argued by Bergstra and Bengio in [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf), â€œrandomly chosen trials are more efficient for hyper-parameter optimization than trials on a gridâ€. As it turns out, this is also usually easier to implement.
>
>   <img src="images/gridsearchbad.jpeg" alt="img" style="zoom:50%;" />
>
>   Core illustration from [Random Search for Hyper-Parameter Optimization](http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf) by Bergstra and Bengio. It is very often the case that some of the hyperparameters matter much more than others (e.g. top hyperparam vs. left one in this figure). Performing random search rather than grid search allows you to much more precisely discover good values for the important ones.
>
>   **Careful with best values on border**. Sometimes it can happen that youâ€™re searching for a hyperparameter (e.g. learning rate) in a bad range. For example, suppose we use `learning_rate = 10 ** uniform(-6, 1)`. Once we receive the results, it is important to double check that the final learning rate is not at the edge of this interval, or otherwise you may be missing more optimal hyperparameter setting beyond the interval.
>
>   **Stage your search from coarse to fine**. In practice, it can be helpful to first search in coarse ranges (e.g. 10 ** [-6, 1]), and then depending on where the best results are turning up, narrow the range. Also, it can be helpful to perform the initial coarse search while only training for 1 epoch or even less, because many hyperparameter settings can lead the model to not learn at all, or immediately explode with infinite cost. The second stage could then perform a narrower search with 5 epochs, and the last stage could perform a detailed search in the final range for many more epochs (for example).
>
>   **Bayesian Hyperparameter Optimization** is a whole area of research devoted to coming up with algorithms that try to more efficiently navigate the space of hyperparameters. The core idea is to appropriately balance the exploration - exploitation trade-off when querying the performance at different hyperparameters. Multiple libraries have been developed based on these models as well, among some of the better known ones are [Spearmint](https://github.com/JasperSnoek/spearmint), [SMAC](http://www.cs.ubc.ca/labs/beta/Projects/SMAC/), and [Hyperopt](http://jaberg.github.io/hyperopt/). However, in practical settings with ConvNets it is still relatively difficult to beat random search in a carefully-chosen intervals. See some additional from-the-trenches discussion [here](http://nlpers.blogspot.com/2014/10/hyperparameter-search-bayesian.html).
>
>   --- cs231n



## Evaluation (of Generalization --- the closeness to ground-truth) & Solutions

### æ¦‚å¿µ

åœ¨ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­ï¼Œ**biasï¼ˆåå·®ï¼‰**å’Œ**varianceï¼ˆæ–¹å·®ï¼‰**æ˜¯ä¸¤ä¸ªé‡è¦çš„æ¦‚å¿µï¼Œå®ƒä»¬æ˜¯ç”¨æ¥<u>è¡¡é‡æ¨¡å‹é¢„æµ‹**è¯¯å·®çš„ä¸»è¦æ¥æº**</u>ã€‚åœ¨è®¾è®¡å’Œè®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹æ—¶ï¼Œå­˜åœ¨ä¸€ä¸ªbias-varianceæƒè¡¡çš„é—®é¢˜ã€‚å¦‚æœ**æ¨¡å‹è¿‡äºç®€å•å¹¶ä¸”å…·æœ‰æœ‰é™çš„å‚æ•°ï¼ˆæ¨¡å‹çš„ç‰¹å¾å¤ªå¼±ï¼Œä¸è¶³ä»¥åŒºåˆ†ä¸åŒæ ·æœ¬ï¼Œå¯¹æ ·æœ¬å˜åŒ–ä¸æ•æ„Ÿï¼‰ï¼Œåˆ™å¯èƒ½ä¼šæœ‰é«˜åå·®å’Œä½æ–¹å·®ã€‚åœ¨å¦ä¸€æ–¹é¢ï¼Œå¦‚æœæ¨¡å‹è¿‡äºå¤æ‚å¹¶ä¸”å…·æœ‰å¤§é‡çš„å‚æ•°ï¼ˆæ¨¡å‹çš„ç‰¹å¾å¤ªå¼ºï¼Œè¿‡äºåŒºåˆ†ä¸åŒæ ·æœ¬ï¼Œå¯¹æ ·æœ¬å˜åŒ–å¤ªæ•æ„Ÿï¼‰ï¼Œåˆ™å¯èƒ½ä¼šæœ‰ä½åå·®å’Œé«˜æ–¹å·®ã€‚**åœ¨è¿™ä¸¤ç§æƒ…å†µä¸‹ï¼Œæ¨¡å‹åœ¨æµ‹è¯•é›†ä¸Šçš„è¡¨ç°éƒ½å¯èƒ½ä¸ä½³ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯**æ‰¾åˆ°åå·®å’Œæ–¹å·®ä¹‹é—´çš„æœ€ä½³å¹³è¡¡ç‚¹ï¼Œä»¥å®ç°æœ€ä½³çš„æ³›åŒ–èƒ½åŠ›**ã€‚

1.  **Biasï¼ˆåå·®ï¼‰**ï¼šåå·®æ˜¯æŒ‡æ¨¡å‹çš„é¢„æµ‹å€¼ä¸å®é™…å€¼çš„å¹³å‡å·®å¼‚ï¼Œæˆ–è€…è¯´æ˜¯æ¨¡å‹çš„é¢„æœŸé¢„æµ‹ä¸çœŸå®é¢„æµ‹ä¹‹é—´çš„å·®è·ã€‚ç®€å•æ¥è¯´ï¼Œé«˜åå·®æ¨¡å‹ä¼šå¿½è§†æ•°æ®ä¸­çš„ç»†èŠ‚ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´æ¬ æ‹Ÿåˆï¼ˆunderfittingï¼‰ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒé›†å’Œæµ‹è¯•é›†ä¸Šçš„è¡¨ç°éƒ½ä¸å¥½ã€‚

    <img src="images/image-20230724173953912.png" alt="image-20230724173953912" style="zoom:50%;" />

    *($\mu$çš„ä¸€é˜¶çŸ©æ˜¯æ— åä¼°è®¡)*  

2.  **Varianceï¼ˆæ–¹å·®ï¼‰/ Deviationï¼ˆæ ‡å‡†å·®ï¼‰**ï¼šæ–¹å·®æ˜¯æŒ‡æ¨¡å‹é¢„æµ‹çš„å˜åŒ–æ€§æˆ–ç¦»æ•£ç¨‹åº¦ï¼Œå³åŒæ ·å¤§å°çš„ä¸åŒè®­ç»ƒé›†è®­ç»ƒå‡ºçš„æ¨¡å‹çš„é¢„æµ‹ç»“æœçš„å˜åŒ–ã€‚å¦‚æœæ¨¡å‹å¯¹è®­ç»ƒæ•°æ®çš„å°å˜åŠ¨éå¸¸æ•æ„Ÿï¼Œé‚£ä¹ˆæ¨¡å‹å°±æœ‰é«˜æ–¹å·®ï¼Œè¿™é€šå¸¸ä¼šå¯¼è‡´è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ï¼Œå³æ¨¡å‹åœ¨è®­ç»ƒé›†ä¸Šè¡¨ç°å¾ˆå¥½ï¼Œä½†åœ¨æµ‹è¯•é›†ï¼ˆå³æœªè§è¿‡çš„æ•°æ®ï¼‰ä¸Šè¡¨ç°å·®ã€‚

    <img src="images/image-20230724174527318.png" alt="image-20230724174527318" style="zoom:50%;" />

    *ï¼ˆ$\sigma$çš„ä¸€é˜¶çŸ©æ˜¯æœ‰åä¼°è®¡ï¼‰*

    æ‰¾åŒä¸€å‡½æ•°çš„æœ€ä½³å‚æ•°ç›¸å½“äºå¯¹ç€é¶å­å¼€ä¸€æªï¼Œ**åå·®**ç›¸å½“äºç„å‡†ç‚¹åç¦»äº†å®é™…é¶å¿ƒï¼Œ**æ–¹å·®**ç›¸å½“äºå°„å‡»ç‚¹åç¦»äº†ç„å‡†ç‚¹ï¼š

    <img src="images/image-20230724174659994.png" alt="image-20230724174659994" style="zoom: 33%;" />

    <img src="images/image-20230724175124356.png" alt="image-20230724175124356" style="zoom: 40%;" />

    ï¼ˆä¸åŒçš„ $f$ å¯¹åº”ç”¨ä¸åŒæ•°æ®ç‚¹è®­ç»ƒå‡ºçš„åŒä¸€å‡½æ•°çš„ä¸åŒå‚æ•°ï¼‰

    ä¸åŒå…¬å¼çš„å‡½æ•°çš„è®­ç»ƒç»“æœåˆ†å¸ƒå›¾ï¼š

    <img src="images/image-20230724175523663.png" alt="image-20230724175523663" style="zoom:40%;" />

    è¶Šå¤æ‚çš„å‡½æ•°å—æ•°æ®é›†å˜åŒ–çš„å½±å“è¶Šå¤§ï¼Œå˜åŠ¨å¹…åº¦è¶Šå¤§ï¼Œä¸åŒ â€œuniverseâ€ çš„è®­ç»ƒç»“æœé—´å·®è·æ›´å¤§ï¼Œå³æ–¹å·®æ›´å¤§ï¼š

    <img src="images/image-20230724175711069.png" alt="image-20230724175711069" style="zoom:40%;" />

    ä½†æ–¹å·®å¤§æ—¶è¦†ç›–çš„èŒƒå›´ä¹Ÿè¶Šå¤§ï¼Œå–å‡å€¼ï¼ˆä¸€é˜¶çŸ©ï¼‰æ—¶æ›´å®¹æ˜“è¦†ç›–â€œé¶å¿ƒâ€ï¼Œå› æ­¤è¶Šå¤æ‚çš„æ¨¡å‹åå·®è¶Šå°ï¼š

    <img src="images/image-20230724180100173.png" alt="image-20230724180100173" style="zoom:40%;" />

    **ç»“è®ºï¼šå•å±‚å‚æ•°å¤šã€éçº¿æ€§çš„æ¨¡å‹ï¼ˆå•ä¸ªå¤æ‚å‡½æ•°ï¼‰ï¼Œæœ€ç†æƒ³ä¼˜åŒ–ç»“æœå’ŒçœŸå®æƒ…å†µé—´ Loss ï¼ˆå³Biasï¼‰æ›´å°ï¼Œä½†å®é™…çš„ training å¾ˆéš¾æ‰¾åˆ°å¥½çš„ç»“æœï¼Œå› ä¸ºå‚æ•°å¤šæ„å‘³ç€å‡½æ•°ç©ºé—´æ›´å¤§æ›´éš¾ä¼˜åŒ–ï¼Œä¸”éçº¿æ€§å‡½æ•°æ¯”çº¿æ€§å‡½æ•°éš¾ä¼˜åŒ– ã€‚**

### è§£å†³

**è™½ç„¶ç›´æ¥ä¼˜åŒ–ç›®æ ‡æ˜¯Losså°ï¼Œè€Œéåå·®ã€æ–¹å·®æ˜¯å¦æœ€ä½³å¹³è¡¡ï¼ˆè¿™ä¹Ÿå¾ˆéš¾ä½œä¸ºä¼˜åŒ–ç›®æ ‡ï¼‰ï¼Œ**<u>ä½†ä¸€èˆ¬è®­ç»ƒé›†Lossè¶³å¤Ÿå°ä¸”æµ‹è¯•é›†Lossä¸ä¸¥é‡é«˜äºè®­ç»ƒé›†ï¼Œå°±æ˜¯æ¯”è¾ƒå¹³è¡¡çš„ç»“æœ</u>ã€‚

ä¸€èˆ¬æ˜¯é’ˆå¯¹å¤§æ–¹å·®åšæ”¹è¿›ï¼š

-   æ›´å¤šè®­ç»ƒæ•°æ®ï¼Œä»¥è®©æ¨¡å‹å­¦ä¹ æ›´ä¸°å¯Œçš„ç‰¹å¾ï¼Œå…‹æœå¤§æ–¹å·®
-   å„ç§æ­£åˆ™åŒ–ï¼Œæå‰æƒ©ç½šæ¨¡å‹ä»¥é˜²æ­¢ overfitï¼ˆè§ä¸‹ä¸€èŠ‚ï¼‰
-   **æ”¹è¿›æ¨¡å‹ï¼šä½¿ç”¨å¤šå±‚çº¿æ€§å‡½æ•°ï¼ˆæ·±åº¦ç¥ç»ç½‘ç»œï¼‰ï¼Œè€Œéå•å±‚éçº¿æ€§å‡½æ•°**ï¼Œè¿™æ ·ä½¿ç”¨åå‘ä¼ æ’­åˆ†åˆ«æ›´æ–°å„å±‚ï¼ˆå„ç»´åº¦ï¼‰çš„å‚æ•°ï¼Œå°±æ¯”å•å±‚çš„å¾ˆå¤šå‚æ•°éƒ½åœ¨åŒä¸€ç»´åº¦æ›´å®¹æ˜“è®­ç»ƒ

### æ·±åº¦ç¥ç»ç½‘ç»œçš„é—®é¢˜

-   è¿˜æ˜¯é—ç•™äº†çº¿æ€§å‡½æ•°åå·®å¤§çš„é—®é¢˜ï¼ˆ**ç¥ç»ç½‘ç»œé€€åŒ–**ï¼‰

    ç†æƒ³ï¼šï¼ˆä½†å®é™…ä¸å¯èƒ½é—­åˆåµŒå¥—ï¼Œå› ä¸ºéƒ½æ˜¯çº¿æ€§å‡½æ•°ï¼‰
    ![image-20240209151507815](images/image-20240209151507815.png)å®é™…ï¼š

    ![image-20240209151550333](image-20240209151550333.png) 

-   æ­¤å¤–è¿˜æœ‰ç½‘ç»œå¤ªæ·±æ¢¯åº¦é™åˆ°0çš„é—®é¢˜ï¼ˆ**æ¢¯åº¦æ¶ˆå¤±**ï¼‰

### å†æ¬¡è§£å†³

***å¯åœ¨æ¨¡å‹æ¶æ„ä¸­æ·»åŠ  ResBlock ï¼ˆè§ [Modules](.\Modules.md)ï¼‰è§£å†³ =ã€‹å¼ºåˆ¶è¦æ±‚ä¸‹å±‚ç½‘ç»œåµŒå¥—ï¼ˆå®Œå…¨è¦†ç›–ï¼‰ä¸Šå±‚***ï¼ˆä¾ç„¶ä¸æ˜¯é—­åˆåµŒå¥—ï¼Œåªæ˜¯å¼ºè¡ŒåŠ ä¸Šè¾“å…¥çš„ç©ºé—´ä½œä¸ºè‡ªå·±çš„æ‹–å¸¦ç©ºé—´ï¼‰ ï¼ŒåŒæ—¶ä¹ŸæŠµå¾¡äº†æ¢¯åº¦æ¶ˆå¤±é—®é¢˜

### æ€»ç»“

![0971bf528ff500b33469ba7387ac3c8](images/image-20240209153821466.png)



## Deal With Over-fitting

***é¢„æµ‹ç»“æœä¸å¥½ï¼ˆå‡†ç¡®åº¦/ æŸå¤±å€¼ï¼‰ä½†è®­ç»ƒç»“æœæ›´å¥½ â€”â€” è¿‡æ‹Ÿåˆ***

![image-20230812120700025](images/image-20230812120700025.png)

### Early Stopping

å‡†å¤‡éªŒè¯é›†ï¼ˆä¸€èˆ¬æ˜¯æ‰€æœ‰å·²çŸ¥æ•°æ®**è®­ç»ƒ:éªŒè¯=8:2**ï¼‰ï¼Œä¸€è¾¹è®­ç»ƒä»¥æ›´æ–°å‚æ•°ä¸€è¾¹åœ¨æ¯ä¸ªepochç»“æŸç”¨éªŒè¯é›†æ±‚è¯¯å·®ï¼ˆè®­ç»ƒè¯¯å·®ä¹Ÿæ±‚å‡ºæ¥ï¼Œç”¨äºæ¯”è¾ƒè§‚å¯Ÿæ˜¯å¦è¿‡æ‹Ÿåˆï¼‰ï¼š

-   ä»¥éªŒè¯é›†è¯¯å·®å°äºæŸé˜ˆå€¼æˆ–æ›´æ–°å¤šå°‘æ¬¡åä¸å†å‡ºç°æ›´å°å€¼ä¸º early stopping æ ‡å¿—ï¼›
-   ä»¥éªŒè¯é›†è¯¯å·®åœ¨å¤šå°‘æ¬¡åä¸å†å°äºç­‰äºè®­ç»ƒé›†è¯¯å·®ä¸ºæ ‡å¿—ã€‚

æœŸé—´ä¿ç•™<u>ä½¿éªŒè¯é›†è¯¯å·®æœ€å°</u>çš„å‚æ•°ç›´åˆ°æœ€åã€‚

### Dropout

***detail see https://cs231n.github.io/neural-networks-2/#reg***

ç±»ä¼¼å†³ç­–æ ‘çš„å‰ªæï¼Œä½† Dropout ä¸€èˆ¬æ˜¯æŒ‰æ¯”ä¾‹éšæœºå…³é—­ç¥ç»å…ƒï¼ˆæ¯ä¸ª$W$ä¸­çš„æŸäº›`input_dim`çš„å‘é‡\<ä¸€æ•´åˆ—è®¾ä¸º0>ï¼‰ï¼Œå‰ªææ˜¯ç»æ³›åŒ–æ€§èƒ½çš„æ¯”è¾ƒåå‡å»å†³ç­–åˆ†æ”¯

>   **Dropout** is an extremely effective, simple and recently introduced regularization technique by Srivastava et al. in [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) (pdf) that complements the other methods (L1, L2, maxnorm). While training, dropout is implemented by only keeping a neuron active with some probability $p$ (a hyperparameter), or setting it to zero otherwise.
>
>   --- *cs231n*

![image-20230825153900438](images/image-20230825153900438.png)

ç®€å•åšæ³•ï¼šå•çº¯åœ¨è®­ç»ƒæ—¶ä»¥ $p$ çš„æ¦‚ç‡å…³é—­ç¥ç»å…ƒè®­ç»ƒï¼Œä½†ç”±äºè¿™æ ·è®­ç»ƒåæ¯ä¸ªç¥ç»å…ƒçš„è¾“å‡ºä¸º 
$$
out'=p\cdot out+(1-p)\cdot0
$$
æ‰€ä»¥ç”¨å®Œæ•´ç½‘ç»œéªŒè¯/æµ‹è¯•æ—¶ï¼Œä¸ºäº†å‘æŒ¥å‡ºç”¨ $Dropout$ è®­ç»ƒçš„æ¨¡å‹çš„æ€§èƒ½ï¼ˆè¾¾åˆ°è¾“å‡ºä¸º $p\cdot out+(1-p)\cdot0$ çš„æ•ˆæœï¼‰ï¼Œ**éœ€è¦æŠŠåŸå§‹è¾“å‡ºå±‚å…¨éƒ¨ç¼©å°åˆ° $p$ å€ï¼š**

```python
""" Vanilla Dropout: Not recommended implementation """

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  """ X contains the data """
  
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = np.random.rand(*H1.shape) < p # first dropout mask
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = np.random.rand(*H2.shape) < p # second dropout mask
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) * p # NOTE: scale the activations
  H2 = np.maximum(0, np.dot(W2, H1) + b2) * p # NOTE: scale the activations
  out = np.dot(W3, H2) + b3
```

âœ¨ ä¼˜è§£ï¼šç›´æ¥åœ¨è®­ç»ƒä½¿ç”¨ $Dropout$ çš„åŒæ—¶**å°†è¾“å‡ºå±‚æ‰©å¤§åˆ° $1/p$ å€**ï¼ŒéªŒè¯/æµ‹è¯•æ—¶æ— éœ€ä»»ä½•æ”¹åŠ¨ï¼Œæ›´æ–¹ä¾¿ï¼š

```python
""" 
Inverted Dropout: Recommended implementation example.
We drop and scale at train time and don't do anything at test time.
"""

p = 0.5 # probability of keeping a unit active. higher = less dropout

def train_step(X):
  # forward pass for example 3-layer neural network
  H1 = np.maximum(0, np.dot(W1, X) + b1)
  U1 = (np.random.rand(*H1.shape) < p) / p # first dropout mask. Notice /p!
  H1 *= U1 # drop!
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  U2 = (np.random.rand(*H2.shape) < p) / p # second dropout mask. Notice /p!
  H2 *= U2 # drop!
  out = np.dot(W3, H2) + b3
  
  # backward pass: compute gradients... (not shown)
  # perform parameter update... (not shown)
  
def predict(X):
  # ensembled forward pass
  H1 = np.maximum(0, np.dot(W1, X) + b1) # no scaling necessary
  H2 = np.maximum(0, np.dot(W2, H1) + b2)
  out = np.dot(W3, H2) + b3
```

### Shuffle

æ¯ä¸ªepochå¼€å§‹å‰å¯¹batchesæ´—ç‰Œï¼ˆåœ¨ `DataLoader` å¤„è®¾ç½®ï¼Œè®­ç»ƒé›†å¼€å¯ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†å…³é—­ï¼‰

### Maximize the Margin + Soft Margin

åœ¨åˆ†ç±»é—®é¢˜ä¸­ï¼Œæœ€å¤§åŒ–æœ‰è®­ç»ƒé›†å­¦å¾—çš„åˆ’åˆ†è¶…å¹³é¢ä¸¤ä¾§çš„é—´éš”ï¼ˆå‚è€ƒã€Šæœºå™¨å­¦ä¹ ã€‹æ”¯æŒå‘é‡æœºï¼‰ï¼Œå¹¶ä¸”å…è®¸éƒ¨åˆ†è®­ç»ƒæ•°æ®å‡ºé”™ï¼Œå³è½å…¥é—´éš”å†…è€Œä¸åœ¨ç¡¬é—´éš”å¤–éƒ¨çš„ä¸¥æ ¼åˆ†ç±»åŒºåŸŸï¼ˆè½¯åŒ–é—´éš”ï¼‰ã€‚

### Max norm constraints 

>   Another form of regularization is to enforce an absolute upper bound on the magnitude of the weight vector for every neuron and use projected gradient descent to enforce the constraint. In practice, this corresponds to performing the parameter update as normal, and then **enforcing the constraint by clamping the weight vector $\vec W$ of every neuron to satisfy $||\vec W||_2<c$. Typical values of $c$ are on orders of 3 or 4.** Some people report improvements when using this form of regularization. One of its appealing properties is that **network cannot â€œexplodeâ€ even when the learning rates are set too high because the updates are always bounded.**
>
>   --- *cs231n*

### Support Vector Regression (SVR)

åœ¨å›å½’é—®é¢˜ä¸­ï¼Œå®¹å¿è½åœ¨é—´éš”å¸¦å†…çš„å‡ºé”™æ•°æ®ï¼Œä¸è®¡ç®—å…¶è¯¯å·®åˆ° Loss ï¼Œå³ç»™ Loss Function æ·»åŠ ä¸€ä¸ªä¸æ•æ„ŸæŸå¤±å‡½æ•°é¡¹ï¼Œç±»ä¼¼æ­£åˆ™åŒ–ï¼ˆå‚è€ƒã€Šæœºå™¨å­¦ä¹ ã€‹æ”¯æŒå‘é‡æœºï¼‰ã€‚

### L2 Regularization (Weight-decay)

See [Loss Function](D:\CAMPUS\AI\MachineLearning\LossFunction.md)

***æ­£åˆ™é¡¹æœ¬è´¨ä¸Šæ˜¯æ”¯æŒå‘é‡æœºçš„æœ€å¤§åŒ–é—´éš”æ³•äº§ç”Ÿçš„ï¼Œå…¶è¡¨ç¤ºçš„å…¶å®æ˜¯åˆ’åˆ†è¶…å¹³é¢ä¸¤ä¾§å®‰å…¨é—´éš”çš„æ€»å¤§å°ï¼ˆå¾…æœ€å¤§åŒ–ï¼‰ï¼Œå‚è€ƒã€Šæœºå™¨å­¦ä¹ ã€‹p122ï¼ˆä¾§æ ç¬”è®°æœ‰æ¨å¯¼ï¼‰,123,133*** 

### L1 Regularization

æ¯” $L2$ æ›´é¼“åŠ± $W$ ä¸ºç¨€ç–çŸ©é˜µï¼Œæ­£åˆ™é¡¹ä¸º $W$ çš„ $L1$ è·ç¦»

å›¾åƒç”Ÿæˆç”¨L2ä¼šæ•æ‰åˆ°æ›´å¤šè¿‡æ‹Ÿåˆï¼ˆéæ³›åŒ–ï¼‰çš„ç»†èŠ‚ï¼ˆå¦‚ç”±åŸå›¾ç”ŸæˆåŸå›¾ä¸å­˜åœ¨çš„æ¥è‡ªå…¶ä»–åŸå›¾çš„äº‹ç‰©ã€ç”Ÿæˆæ— æ„ä¹‰è‚Œç†ç­‰ï¼‰ï¼Œåä¹‹L1åªä¼šæ•æ‰æ›´æ³›åŒ–çš„ç‰¹å¾ä½†å›¾åƒè½®å»“ä¼šè¾ƒæ¨¡ç³Š

### (In Classification) Weaken Features of a Each Class

ä¼°è®¡å„ç±»åˆ«çš„åˆ†å¸ƒæ—¶ï¼ˆå»ºæ¨¡æ—¶ï¼‰ï¼Œ**ä½¿å®ƒä»¬æ‹¥æœ‰ç›¸åŒçš„æŸå‚æ•°**ï¼Œè¯¥å‚æ•°<u>ç”±å„ç±»åˆ«åˆ†åˆ«ç”¨å¯¹åº”è®­ç»ƒé›†ä¼°è®¡å‡ºçš„å‚æ•°å†å¯¹æ ·æœ¬æ•°**åŠ æƒå¹³å‡**å¾—åˆ°</u>

### Simplify Network Structure to Cut Down Sensitivity of  Features' Alternation

See **<u>CNN</u>**

### Batch Normalization

See  [DataProcessing](D:\CAMPUS\AI\MachineLearning\ML_MDnotes\DataProcessing.md)---**<u>Batch Normalization</u>*

### KL Regularization, VQ Regularization

See [VAE & VQVAE](https://zhouyifan.net/2024/01/23/20230707-SD1/)



## Stable Training

```python
# Clip the gradient norms for stable training.
        grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)
```

`nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)`è¿™ä¸€è¡Œä»£ç ç”¨äºæ¢¯åº¦è£å‰ªï¼Œå…¶ç›®çš„æ˜¯ä¸ºäº†é˜²æ­¢åœ¨ç¥ç»ç½‘ç»œè®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°"æ¢¯åº¦çˆ†ç‚¸"çš„é—®é¢˜ã€‚

æ¢¯åº¦çˆ†ç‚¸æ˜¯æŒ‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ¨¡å‹çš„å‚æ•°çš„æ¢¯åº¦å˜å¾—éå¸¸å¤§ï¼Œä»¥è‡³äºæ›´æ–°æ­¥é•¿è¿‡å¤§ï¼Œå¯¼è‡´æ¨¡å‹æ— æ³•æ”¶æ•›ï¼Œç”šè‡³ç»“æœæº¢å‡ºã€‚å½“è®¡ç®—å‡ºçš„æ¢¯åº¦å¤§äºæŸä¸ªè®¾å®šçš„æœ€å¤§å€¼ï¼ˆåœ¨è¿™é‡Œæ˜¯10ï¼‰æ—¶ï¼Œè¿™ä¸ªå‡½æ•°å°†ä¼šæŒ‰æ¯”ä¾‹ç¼©å°æ¢¯åº¦ï¼Œä½¿å¾—å…¶ä¸è¶…è¿‡è¿™ä¸ªæœ€å¤§å€¼ï¼Œä»è€Œé¿å…æ¢¯åº¦çˆ†ç‚¸ã€‚

`grad_norm = nn.utils.clip_grad_norm_(model.parameters(), max_norm=10)`è¿™è¡Œä»£ç ä¼šè¿”å›æ‰€æœ‰æ¨¡å‹å‚æ•°æ¢¯åº¦çš„L2èŒƒæ•°ã€‚

æ¢¯åº¦è£å‰ªæ˜¯å¾ˆå¸¸è§çš„åœ¨è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯åœ¨è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œå’Œå¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRNNï¼‰æ—¶ï¼Œç”¨äºä¿æŒæ¨¡å‹çš„ç¨³å®šæ€§ã€‚



## Ensemble

***ï¼ˆè¯¦è§ã€Šæœºå™¨å­¦ä¹ ã€‹ç¬¬8ç« ï¼‰***

**"Ensemble"ï¼ˆé›†æˆï¼‰**æ˜¯ä¸€ç§æœºå™¨å­¦ä¹ ç­–ç•¥ï¼Œå®ƒç»“åˆäº†å¤šä¸ªæ¨¡å‹çš„é¢„æµ‹ï¼Œä»¥äº§ç”Ÿä¸€ä¸ªæœ€ç»ˆçš„é¢„æµ‹ç»“æœã€‚è¿™ç§æ–¹æ³•çš„ä¸»è¦ç›®æ ‡æ˜¯é€šè¿‡ç»„åˆå¤šä¸ªæ¨¡å‹æ¥å‡å°å•ä¸€æ¨¡å‹çš„é¢„æµ‹è¯¯å·®ã€‚

ä¸»è¦çš„é›†æˆæ–¹æ³•æœ‰ï¼š

1.  Baggingï¼ˆBootstrap Aggregatingï¼‰ï¼šé€šè¿‡ä»è®­ç»ƒæ•°æ®ä¸­<u>**æœ‰æ”¾å›åœ°éšæœºæŠ½æ ·ï¼ˆè‡ªåŠ©é‡‡æ ·æ³•ï¼‰**æ¥ç”Ÿæˆä¸åŒçš„è®­ç»ƒé›†ï¼Œç„¶ååœ¨æ¯ä¸ªè®­ç»ƒé›†ä¸Šè®­ç»ƒä¸€ä¸ªæ¨¡å‹</u>ã€‚æœ€åï¼Œæ‰€æœ‰æ¨¡å‹çš„é¢„æµ‹ç»“æœ**é€šè¿‡æŠ•ç¥¨**ï¼ˆå¤šæ•°èƒœå‡ºï¼›åˆ†ç±»é—®é¢˜ï¼‰æˆ–**å¹³å‡**ï¼ˆå›å½’é—®é¢˜ï¼‰æ¥äº§ç”Ÿæœ€ç»ˆé¢„æµ‹ã€‚**éšæœºæ£®æ—ï¼ˆRandom Forestï¼‰**å°±æ˜¯ä¸€ä¸ªå…¸å‹çš„Baggingçš„ä¾‹å­ã€‚

2.  **Boosting**ï¼šä¸€ç§è¿­ä»£çš„ç­–ç•¥ï¼Œå…¶ä¸­æ–°æ¨¡å‹çš„è®­ç»ƒä¾èµ–äºä¹‹å‰æ¨¡å‹çš„æ€§èƒ½ã€‚æ¯ä¸ªæ–°æ¨¡å‹éƒ½è¯•å›¾ä¿®æ­£ä¹‹å‰æ¨¡å‹çš„é”™è¯¯ã€‚æœ€è‘—åçš„Boostingç®—æ³•æœ‰AdaBoostå’Œæ¢¯åº¦æå‡ã€‚

    >   *å…ˆä»åˆå§‹è®­ç»ƒé›†è®­ç»ƒå‡ºä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼Œå†æ ¹æ®åŸºå­¦ä¹ å™¨çš„è¡¨ç°å¯¹è®­ç»ƒæ ·æœ¬åˆ†å¸ƒè¿›è¡Œè°ƒæ•´ï¼Œä½¿å¾—å…ˆå‰åŸºå­¦ä¹ å™¨åšé”™çš„è®­ç»ƒæ ·æœ¬åœ¨åç»­å—åˆ°æ›´å¤šå…³æ³¨ï¼Œç„¶ååŸºäºè°ƒæ•´åçš„æ ·æœ¬åˆ†å¸ƒæ¥è®­ç»ƒä¸‹ä¸€ä¸ªåŸºå­¦ä¹ å™¨ï¼›å¦‚æ­¤é‡å¤è¿›è¡Œï¼Œç›´è‡³åŸºå­¦ä¹ å™¨æ•°ç›®è¾¾åˆ°äº‹å…ˆæŒ‡å®šçš„å€¼ $T$ï¼Œæœ€ç»ˆå°†è¿™ $T$ ä¸ªåŸºå­¦ä¹ å™¨è¿›è¡ŒåŠ æƒç»“åˆã€‚*
    >
    >   *â€”â€” ã€Šæœºå™¨å­¦ä¹ ã€‹P173*
    >
    >   ***è°ƒæ•´æ ·æœ¬åˆ†å¸ƒåŒ…æ‹¬<u>é‡èµ‹æƒæ³•ï¼ˆé€‚ç”¨äºæ ·æœ¬å¯å¸¦æƒå­¦ä¹ ç®—æ³•ã€å­¦ä¹ è¿‡ç¨‹æœªå› é‡åˆ°ä¸æ»¡è¶³åŸºæœ¬æ¡ä»¶çš„åŸºå­¦ä¹ å™¨è€Œæå‰åœæ­¢çš„æƒ…å†µï¼‰</u>å’Œ<u>é‡é‡‡æ ·æ³•ï¼ˆé€‚ç”¨äºä¸å¯å¸¦æƒå­¦ä¹ ç®—æ³•ã€å­¦ä¹ è¿‡ç¨‹å› é‡åˆ°ä¸æ»¡è¶³åŸºæœ¬æ¡ä»¶çš„åŸºå­¦ä¹ å™¨è€Œæå‰åœæ­¢çš„æƒ…å†µï¼‰</u>***

3.  Stackingï¼šä½¿ç”¨å¤šä¸ªæ¨¡å‹æ¥è®­ç»ƒæ•°æ®ï¼Œç„¶åå†ç”¨ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼ˆå«åšå…ƒæ¨¡å‹æˆ–è€…äºŒçº§æ¨¡å‹ï¼‰æ¥é¢„æµ‹è¿™äº›æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚

åœ¨è®¸å¤šæœºå™¨å­¦ä¹ ç«èµ›ä¸­ï¼Œé›†æˆå­¦ä¹ è¢«è¯æ˜æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆçš„æ–¹æ³•ï¼Œå› ä¸ºå®ƒå¯ä»¥**å‡å°‘è¿‡æ‹Ÿåˆï¼Œå¢åŠ æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›ï¼Œä»è€Œæé«˜é¢„æµ‹æ€§èƒ½**ã€‚

>   In practice, one reliable approach to improving the performance of Neural Networks by a few percent is to train multiple independent models, and at test time average their predictions. As the number of models in the ensemble increases, the performance typically monotonically improves (though with diminishing returns). Moreover, the improvements are more dramatic with higher model variety in the ensemble. There are a few approaches to forming an ensemble:
>
>   -   **Same model, different initializations**. Use cross-validation to determine the best hyperparameters, then train multiple models with the best set of hyperparameters but with different random initialization. The danger with this approach is that the variety is only due to initialization.
>   -   **Top models discovered during cross-validation**. Use cross-validation to determine the best hyperparameters, then pick the top few (e.g. 10) models to form the ensemble. This improves the variety of the ensemble but has the danger of including suboptimal models. In practice, this can be easier to perform since it doesnâ€™t require additional retraining of models after cross-validation
>   -   **Different checkpoints of a single model**. If training is very expensive, some people have had limited success in taking different checkpoints of a single network over time (for example after every epoch) and using those to form an ensemble. Clearly, this suffers from some lack of variety, but can still work reasonably well in practice. The advantage of this approach is that is very cheap.
>   -   **Running average of parameters during training**. Related to the last point, a cheap way of almost always getting an extra percent or two of performance is to maintain a second copy of the networkâ€™s weights in memory that maintains an exponentially decaying sum of previous weights during training. This way youâ€™re averaging the state of the network over last several iterations. You will find that this â€œsmoothedâ€ version of the weights over last few steps almost always achieves better validation error. The rough intuition to have in mind is that the objective is bowl-shaped and your network is jumping around the mode, so the average has a higher chance of being somewhere nearer the mode.
>
>   One disadvantage of model ensembles is that they take longer to evaluate on test example. An interested reader may find the recent work from Geoff Hinton on [â€œDark Knowledgeâ€](https://www.youtube.com/watch?v=EK61htlw8hY) inspiring, where the idea is to â€œdistillâ€ a good ensemble back to a single model by incorporating the ensemble log likelihoods into a modified objective.
>
>   --- *cs231n*



## Why Deep?????????

-   åœ¨æ¯”æµ…å±‚æ¨¡å‹çš„å„å±‚å‚æ•°ä¸ªæ•°å°‘çš„æƒ…å†µä¸‹ï¼Œé€šè¿‡å¢åŠ æ·±åº¦æ¥å¢åŠ æ¨¡å‹å¤æ‚åº¦ä»¥å‡å°åå·®ï¼Œä¸”æœ€ç»ˆæ•´ä½“å‚æ•°ä¸ªæ•°ä¹Ÿå°‘äºæµ…å±‚ä½†å„å±‚å‚æ•°æ›´å¤šçš„æ¨¡å‹ã€å¤æ‚åŒ–è¿‡ç¨‹å¯¹å‚æ•°çš„åˆ©ç”¨ç‡æ›´é«˜ã€‘ï¼Œè¿™æ ·è®­ç»ƒéœ€è¦çš„æ•°æ®ä¹Ÿæ›´å°‘ï¼ˆåœ¨çœŸå®æƒ…å†µå¤æ‚åˆæœ‰æ½œåœ¨è§„å¾‹\<æ¯”å¦‚è¯­éŸ³ã€å›¾åƒçš„æ¨¡å‹>æ—¶æ•ˆæœæ›´çªå‡ºï¼ï¼ï¼ï¼‰
-   åˆ©ç”¨å¤šå±‚æ¿€æ´»å‡½æ•°é€å±‚å¼•å…¥éçº¿æ€§é€¼è¿‘å¤æ‚çš„å‡½æ•°

## Basic Classifiers

### Linear Classifier ï¼ˆå³æŠŠ Linear Regression çš„æ¨¡å‹ç”¨æ¥åˆ†ç±»ï¼ŒæŠŠåˆ†ç±»å½“çº¿æ€§å›å½’åšï¼‰

$$
\vec S_i = W\cdot \vec x_i\ +\ \vec b
$$

$\vec S_i$ æ˜¯ç¬¬ i ä¸ªæ ·æœ¬åœ¨å„ä¸ªç±»åˆ«ä¸­è·å¾—çš„åˆ†æ•°ç»„æˆçš„å‘é‡**ï¼ˆUnnormalized probabilities of Classesï¼‰**ï¼Œ$W$ å¾…å­¦ä¹ çš„çŸ©é˜µï¼Œ$\vec b$ æ˜¯å¾…å­¦ä¹ çš„åå·®å‘é‡ï¼Œä¸å…·ä½“è®­ç»ƒé›†ä¸­ä¸åŒç±»åˆ«çš„æ ·æœ¬çš„æ•°é‡å·®å¼‚æœ‰å…³ï¼ˆå­¦ä¹ æ—¶å°†å¹³è¡¡è¿™æ ·çš„å·®å¼‚ï¼‰ã€‚

ä¼˜åŒ–ç›®æ ‡ï¼šä½¿å¾—çœŸå®labelå¯¹åº”çš„ç±»åˆ«è·å¾—çš„åˆ†æ•°æœ€é«˜

<u>**SVM** ä½¿ç”¨çš„æ¨¡å‹å°±æ˜¯çº¿æ€§æ¨¡å‹ï¼ˆå¯»æ‰¾åˆ’åˆ†è¶…å¹³é¢ï¼‰</u>

****

### Logistic Regression (Discriminative Model; Softmax Classifier)

***ï¼ˆä¸éœ€è¦è‡ªå·±å‡è®¾åˆ†å¸ƒï¼ï¼ï¼ï¼‰***

<img src="images/image-20230812173316594.png" alt="image-20230812173316594" style="zoom:50%;" />

<img src="images/image-20230812172514738.png" alt="image-20230812172514738" style="zoom: 50%;" />

å’Œ Linear Regression çš„åŒºåˆ«ä»…åœ¨<u>**æœ€ç»ˆ**è¾“å‡ºæ—¶**å¤šåŠ ä¸€å±‚ Softmax**</u>ï¼ˆè¿™æ˜¯ Softmax ä½œä¸ºç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒçš„ç”¨é€”æ—¶çš„ç”¨æ³•ï¼Œ<u>è‹¥æ˜¯åˆ«çš„ç”¨é€”åˆ™åœ¨éšè—å±‚ä¹Ÿä¼šåŠ </u>ï¼ï¼ï¼ï¼sigmoidæ˜¯ç‰¹æ®Šçš„Softmaxï¼ŒäºŒåˆ†ç±»æ—¶ç›´æ¥ç”¨sigmoidï¼›**Logit \<å¯¹æ•°å‡ ç‡ï¼Œlog odds>** ç‹­ä¹‰å³æŒ‡ sigmoid çš„åå‡½æ•°ï¼Œå¹¿ä¹‰æŒ‡<u>æœªç» Softmax çš„è¾“å‡º</u>ï¼‰ï¼Œç”¨ Linear Classification è¾“å‡ºçš„åˆ†æ•°ç”Ÿæˆ **Normalized probabilities of Classes**ï¼ˆå±äº (0, 1)ï¼Œå–ä¸åˆ°è¾¹ç•Œï¼‰

æ·»åŠ  Softmax åå› ä¸ºæœ‰äº† (0, 1) çš„ä¸Šä¸‹ç•Œï¼Œé«˜åˆ†æ•°ï¼ˆå‘1æŒ¤å‹ï¼‰ä¼šç›¸è¾ƒä½åˆ†æ•°ï¼ˆå‘0æŒ¤å‹ï¼‰å˜å¾—æ›´é«˜

ä¼˜åŒ–ç›®æ ‡ï¼šä½¿å¾—çœŸå®labelå¯¹åº”çš„ç±»åˆ«è·å¾—çš„åˆ†æ•°æœ€é«˜ => **å³æ¥è¿‘ä¸€ï¼ˆå› ä¸ºå·²ç»è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒäº†ï¼‰**

<img src="images/image-20230812172829902.png" alt="image-20230812172829902" style="zoom:50%;" />

å…¶ä¸­â€œ1â€å¯¹åº”äº¤å‰ç†µæŸå¤±ä¸­çš„å®é™…åˆ†å¸ƒä¸­æ¦‚ç‡ï¼Œâ€œlog(...)â€å¯¹åº”äº¤å‰ç†µæŸå¤±ä¸­çš„ä¼°è®¡åˆ†å¸ƒä¸­æ¦‚ç‡

```python
class My_Model(nn.Module):
    def __init__(self, input_dim, output_dim):
        super().__init__()
        self.layers = nn.Linear(input_dim, output_dim)

    def forward(self, x):
        out = torch.sigmoid(self.linear(x))
        return out
```

**è‹¥åé¢ Loss ç”¨ CrossEntropy åˆ™ Model å°±ç›´æ¥ç”¨ Linear Regression ï¼ï¼ï¼å› ä¸º CrossEntropy å·²åŒ…å« Softmaxï¼ï¼ï¼ï¼ˆè¿™å°±æ˜¯ä¸ºä»€ä¹ˆpytorchæ²¡æœ‰ä¸“é—¨çš„ Logistic Regression å‡½æ•°ï¼‰**

>   **Softmax** çš„ç”¨é€”ï¼š
>
>   -   åœ¨ç½‘ç»œæœ«ç«¯ç”±çº¿æ€§å‡½æ•°çš„è¾“å‡ºç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒï¼ˆæ˜ å°„åˆ° [0,1]ï¼‰
>   -   åœ¨ç½‘ç»œæœ«ç«¯ç”±çº¿æ€§å‡½æ•°çš„è¾“å‡ºç”Ÿæˆæ¦‚ç‡åˆ†å¸ƒä¹‹å‰ï¼Œå¯¹<u>æ¯ä¸ªç‰¹å¾</u>åˆ†åˆ«å®ç°ç‰¹å¾å˜æ¢ä½¿å¾—çº¿æ€§å¯åˆ†ï¼ˆLogistic Regression ç½‘ç»œä¸­ç”¨ï¼‰ã€ä¹Ÿå¯åŠ å¤šå±‚å˜æ¢ã€‘
>   -   åœ¨ç½‘ç»œå†…éƒ¨æ¯å±‚è¾“å‡ºåç”¨ä½œæ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ä½¿å¾—æ¨¡å‹å¯ä»¥æ›´å¥½åœ°é€¼è¿‘å¤æ‚çš„å‡½æ•°ï¼ˆä½†ä¸€èˆ¬ç”¨ReLUï¼‰

#### SVM vs SOFTMAX

![img](images/svmvssoftmax.png)

## Basic Architectures

### Fully connected NN (FCN/ multi-layer feedforward NN) 

```python
class My_Model(nn.Module):  # derived from build-in class "nn.Model"
    def __init__(self, input_dim):
        super().__init__() 
        # structure of the DNN
        self.layers = nn.Sequential(  
            nn.Linear(input_dim, <s1>),
            nn.ReLU(), # activator
            nn.Linear(<s1>, <s2>),
            nn.ReLU(),
            nn.Linear(<s2>, <s3>),
            nn.ReLU(),
            <...>
            nn.Linear(<sn>, output_dim)
        )

    def forward(self, x):  
        x = self.layers(x)  # input a batch of Features (x) to the DNN (in-place)
       #  x = x.squeeze(1)  # (B, 1) -> (B) [simplify the shape of tensor]
        return x
```

****

### CNN ( Convolutional NN)

https://cs231n.github.io/convolutional-networks/#conv

<img src="images/y(1).png" alt="y(1)" style="zoom: 80%;" /> 

#### Basic Form

==***INPUT -> [[CONV -> RELU]\*N -> POOL?]\*M -> [FC -> RELU]\*K -> FC**==  

>   We use three main types of layers to build ConvNet architectures: **Convolutional Layer**, **Pooling Layer**, and **Fully-Connected Layer** (exactly as seen in regular Neural Networks). We will stack these layers to form a full ConvNet **architecture**.
>
>   *A ConvNet is made up of Layers. Every Layer has a simple API: It transforms an input 3D volume to an output 3D volume with some differentiable function that **may or may not have parameters (e.g. CONV/FC do, RELU/POOL donâ€™t)** and also **may or may not have additional hyperparameters (e.g. CONV/FC/POOL do, RELU doesnâ€™t)**.*  
>
>   --- *cs231n*

<img src="images/image-20230901104723724.png" alt="image-20230901104723724" style="zoom:30%;" />

<img src="images/image-20230901144201391.png" alt="image-20230901144201391" style="zoom: 50%;" />

**å›¾åƒåˆ†ç±»å™¨ï¼š **

<img src="images/4e00cbf4065675c605cf7cb04a8805c-1709783349388-2.jpg" alt="4e00cbf4065675c605cf7cb04a8805c" style="zoom:150%;" />

**å˜ä½“ï¼š**å›¾åƒæ•°æ®é€šå¸¸å…ˆå‰ç½®ä¸€å±‚å•ç‹¬çš„ CONV ï¼Œç„¶åå†ä»¥ **RELU-CONV-NORM** ä¸ºå•ä½é‡å 

#### Usage Overview

-   ***ä»ç½‘ç»œå†…éƒ¨ç»“æ„æ”¹å˜æ¨¡å‹æ€§èƒ½***

-   ä¸“ç”¨äºå›¾åƒæ•°æ®ï¼ˆ3ç»´çŸ©é˜µï¼‰ï¼Œ<u>ç½‘ç»œå†…éƒ¨ä¿æŒ3ç»´çš„çŸ©é˜µæ ¼å¼</u>

-   å¦‚è¦æ”¯æŒä»»æ„å¤§å°çš„å›¾åƒè¾“å…¥è€Œä¸æ”¹å˜æ¨¡å‹è¶…å‚æ•°ï¼Œéœ€è¦å‰ç½® **Spatial Transformer Network** 

-   æ®å…·ä½“é—®é¢˜è€ƒè™‘æ˜¯å¦ä½¿ç”¨ **Pooling**ï¼ˆä¸æ˜¯æ‰€æœ‰æ•°æ®éƒ½èƒ½ç”¨ï¼ï¼ä¸€èˆ¬å›¾åƒå¯ä»¥ï¼‰

-   å¯ä»¥æ·»åŠ  **Residual Blockï¼š** ***ã€RNNä¹Ÿå¯ä»¥ç”¨ã€‘é€šè¿‡å°†è¾“å…¥ x ç›´æ¥åŠ åˆ°è¾“å‡º F(x) æ·»åŠ æ¢¯åº¦æ·å¾„***

    ![image-20230909113143212](images/image-20230909113143212.png)

    **ä½œç”¨ï¼šâ‘  å½“æƒé‡æ¶ˆå¤±æ—¶è¾“å‡ºä»æœ‰ x ï¼Œå› æ­¤è‹¥æŸå±‚ç½‘ç»œæ˜¯å†—ä½™çš„ï¼Œå³å½“æŸå¤±å€¼å¾ˆå¤§æ—¶ï¼ˆå°¤å…¶åŠ å…¥æ­£åˆ™é¡¹åï¼‰ï¼Œåœ¨å­¦ä¹ è¿‡ç¨‹ä¸­æƒé‡ä¼šå˜å°ä½†ä»ä¿ç•™å…ˆå‰è¾“å‡ºï¼Œä¸ä¼šç»™æ•´ä¸ªæ¨¡å‹å¸¦æ¥å¤šä½™è´Ÿé¢å½±å“ï¼ˆç›¸å½“äºå…³é—­ä¸éœ€è¦çš„ç½‘ç»œå±‚åŸå°ä¸åŠ¨è¾“å‡ºå…¶è¾“å…¥ï¼‰ï¼›â‘¡ åå‘ä¼ æ’­æ—¶æä¾›äº†æ›´é€šç•…çš„åå¯¼æ”¯è·¯ï¼ˆç±»ä¼¼é«˜é€Ÿå…¬è·¯çš„ä½œç”¨ï¼‰ï¼ŒåŠ é€Ÿç½‘ç»œ converge v é€Ÿåº¦**

-   å…¶ä»–CNNç»“æ„ï¼š

    ![image-20230909115203190](images/image-20230909115203190.png)

äº‹å®ä¸Šä¹Ÿå¯ä»¥ç”¨CNNæŒ‡å‘Fully-Connected Layerå‰é¢çš„éƒ¨åˆ†ï¼ˆåªåŒ…æ‹¬Convolutional Layerå’ŒPooling Layerï¼‰

**IMPORTANT TIPS:**

-   ***Prefer a stack of small filter CONV to one large receptive field CONV layer***

    >   Suppose that you stack three 3x3 CONV layers on top of each other (with non-linearities in between, of course). In this arrangement, each neuron on the first CONV layer has a 3x3 view of the input volume. A neuron on the second CONV layer has a 3x3 view of the first CONV layer, and hence by extension a 5x5 view of the input volume. Similarly, a neuron on the third CONV layer has a 3x3 view of the 2nd CONV layer, and hence a 7x7 view of the input volume. Suppose that instead of these three layers of 3x3 CONV, we only wanted to use a single CONV layer with 7x7 receptive fields. These neurons would have a receptive field size of the input volume that is identical in spatial extent (7x7), but with several disadvantages. First, the neurons would be computing a linear function over the input, while the three stacks of CONV layers contain non-linearities that make their features more expressive. Second, if we suppose that all the volumes have $C$ channels, then it can be seen that the single 7x7 CONV layer would contain $CÃ—(7Ã—7Ã—C)=49C^2$ parameters, while the three 3x3 CONV layers would only contain $3Ã—(CÃ—(3Ã—3Ã—C))=27C^2$ parameters. Intuitively, stacking CONV layers with tiny filters as opposed to having one CONV layer with big filters allows us to express more powerful features of the input, and with fewer parameters. <u>As a practical disadvantage, we might need more memory to hold all the intermediate CONV layer results if we plan to do backpropagation.</u>
    >
    >   

-   ***Use Pretrained CNN from ImageNet for work!!!!!!!*** 

    >   **Recent departures.** It should be noted that the conventional paradigm of a linear list of layers has recently been challenged, in Googleâ€™s Inception architectures and also in current (state of the art) Residual Networks from Microsoft Research Asia. Both of these (see details below in case studies section) feature more intricate and different connectivity structures.
    >
    >   **In practice: use whatever works best on ImageNet**. If youâ€™re feeling a bit of a fatigue in thinking about the architectural decisions, youâ€™ll be pleased to know that in 90% or more of applications you should not have to worry about these. I like to summarize this point as â€œ*donâ€™t be a hero*â€: Instead of rolling your own architecture for a problem, you should look at whatever architecture currently works best on ImageNet, download a pretrained model and finetune it on your data. You should rarely ever have to train a ConvNet from scratch or design one from scratch. I also made this point at the [Deep Learning school](https://www.youtube.com/watch?v=u6aEYuemt0M).
    >
    >   --- *cs231n*

-   ***Input Size:***

    >   The **input layer** (that contains the image) should be divisible by 2 many times. Common numbers include 32 (e.g. CIFAR-10), 64, 96 (e.g. STL-10), or 224 (e.g. common ImageNet ConvNets), 384, and 512.
    >
    >   --- *cs231n*

-   ***Conv Layer Size:***

    >   The **conv layers** should be using small filters (e.g. 3x3 or at most 5x5), using a stride of S=1, and crucially, padding the input volume with zeros in such way that the conv layer does not alter the spatial dimensions of the input. That is, when F=3, then using P=1 will retain the original size of the input. When F=5, P=2. For a general F, it can be seen that P=(Fâˆ’1)/2 preserves the input size. If you must use bigger filter sizes (such as 7x7 or so), it is only common to see this on the very first conv layer that is looking at the input image.
    >
    >   --- *cs231n*

-   ***Pool Layer Size:***

    >   The **pool layers** are in charge of downsampling the spatial dimensions of the input. The most common setting is to use max-pooling with 2x2 receptive fields (i.e. F=2), and with a stride of 2 (i.e. S=2). Note that this discards exactly 75% of the activations in an input volume (due to downsampling by 2 in both width and height). Another slightly less common setting is to use 3x3 receptive fields with a stride of 2, but this makes â€œfittingâ€ more complicated (e.g., a 32x32x3 layer would require zero padding to be used with a max-pooling layer with 3x3 receptive field and stride 2). It is very uncommon to see receptive field sizes for max pooling that are larger than 3 because the pooling is then too lossy and aggressive. This usually leads to worse performance.
    >
    >   --- *cs231n*

-   ***Reducing sizing headaches:*** 

    >   The scheme presented above is pleasing because all the CONV layers preserve the spatial size of their input, while the POOL layers alone are in charge of down-sampling the volumes spatially. In an alternative scheme where we use strides greater than 1 or donâ€™t zero-pad the input in CONV layers, we would have to very carefully keep track of the input volumes throughout the CNN architecture and make sure that all strides and filters â€œwork outâ€, and that the ConvNet architecture is nicely and symmetrically wired.
    >
    >   --- *cs231n*

-   ***Why use stride of 1 in CONV?*** 

    >   Smaller strides work better in practice. Additionally, as already mentioned stride 1 allows us to leave all spatial down-sampling to the POOL layers, with the CONV layers only transforming the input volume depth-wise.
    >
    >   --- *cs231n*

-   ***Why use padding?***

    >   In addition to the aforementioned benefit of keeping the spatial sizes constant after CONV, doing this actually improves performance. <u>If the CONV layers were to not zero-pad the inputs and only perform valid convolutions, then the size of the volumes would reduce by a small amount after each CONV, and the information at the borders would be â€œwashed awayâ€ too quickly.</u>
    >
    >   --- *cs231n*

    ***è™½ç„¶paddingè¡¥çš„æ˜¯0ï¼Œä½†æ˜¯è¿™æ ·å¯ä»¥ä½¿è¾“å…¥å›¾åƒçš„è¾¹ç¼˜ä¹Ÿæœ‰æœºä¼šå……åˆ†æ¥è§¦ Filter çš„å„å¤„æƒé‡***

-    ***Compromising based on <u>memory constraints</u>:***

    >   In some cases (especially early in the ConvNet architectures), the amount of memory can build up very quickly with the rules of thumb presented above. For example, filtering a 224x224x3 image with three 3x3 CONV layers with 64 filters each and padding 1 would create three activation volumes of size [224x224x64]. This amounts to a total of about 10 million activations, or 72MB of memory (per image, for both activations and gradients). Since GPUs are often bottlenecked by memory, it may be necessary to compromise. In practice, people prefer to make the compromise at only the first CONV layer of the network. For example, one compromise might be to use a first CONV layer with filter sizes of 7x7 and stride of 2 (as seen in a ZF net). As another example, an AlexNet uses filter sizes of 11x11 and stride of 4.
    >
    >   --- *cs231n*

-   ***Computational Considerations:***

    >   The largest bottleneck to be aware of when constructing ConvNet architectures is the memory bottleneck. Many modern GPUs have a limit of 3/4/6GB memory, with the best GPUs having about 12GB of memory. There are three major sources of memory to keep track of:
    >
    >   -   From the intermediate volume sizes: These are the raw number of **activations** at every layer of the ConvNet, and also their gradients (of equal size). Usually, most of the activations are on the earlier layers of a ConvNet (i.e. first Conv Layers). These are kept around because they are needed for backpropagation, but a clever implementation that runs a ConvNet only at test time could in principle reduce this by a huge amount, by only storing the current activations at any layer and discarding the previous activations on layers below.
    >   -   From the parameter sizes: These are the numbers that hold the network **parameters**, their gradients during backpropagation, and commonly also a step cache if the optimization is using momentum, Adagrad, or RMSProp. Therefore, the memory to store the parameter vector alone must usually be multiplied by a factor of at least 3 or so.
    >   -   Every ConvNet implementation has to maintain **miscellaneous** memory, such as the image data batches, perhaps their augmented versions, etc.
    >
    >   Once you have a rough estimate of the total number of values (for activations, gradients, and misc), the number should be converted to size in GB. Take the number of values, multiply by 4 to get the raw number of bytes (since every floating point is 4 bytes, or maybe by 8 for double precision), and then divide by 1024 multiple times to get the amount of memory in KB, MB, and finally GB. If your network doesnâ€™t fit, a common heuristic to â€œmake it fitâ€ is to decrease the batch size, since most of the memory is usually consumed by the activations.
    >
    >   --- *cs231n*

#### Advantage

**ç®€åŒ–ç½‘ç»œä½¿å¾—éå…¨è¿æ¥ï¼Œä»¥å‡å°‘ç½‘ç»œå±‚æƒé‡ï¼ˆå‚æ•°ï¼‰æ•°ï¼Œè¿›è€Œå‰Šå¼±ç½‘ç»œçš„ç‰¹å¾æ•æ„Ÿåº¦ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ**

#### A Constraint Rule to Hyper-params

è®¾è¾“å…¥2Dç»´æ•°ä¸º$N^2$ï¼ŒRFçš„2Dç»´æ•°ä¸º$F^2$ï¼ˆ$Hyper$ï¼‰ï¼ŒPaddingå®½åº¦ä¸º$P$ï¼ˆ$Hyper$ï¼‰ï¼Œå·ç§¯æ­¥é•¿ä¸º$S$ï¼ˆ$Hyper$ï¼‰ï¼Œè¾“å‡º2Dç»´æ•°ä¸º$N'^2$ï¼Œå„ç»´æ•°å‡é¡»ä¸ºæ­£æ•´æ•°ï¼Œåˆ™æ’æœ‰ï¼š
$$
N'=\frac{N-F+2P}{S}+1
$$
![e0caa8c2216966d03a45982ffc50b10](images/e0caa8c2216966d03a45982ffc50b10-1693557035121-4.jpg)

#### Special Normalizations for CNN

![image-20230906213347227](images/image-20230906213347227.png)

#### Coding

**`torch.nn.Conv2d`** æ˜¯ PyTorch æä¾›çš„äºŒç»´å·ç§¯æ“ä½œã€‚å·ç§¯åœ¨å›¾åƒå¤„ç†å’Œè®¡ç®—æœºè§†è§‰ä¸­éå¸¸é‡è¦ï¼Œç‰¹åˆ«æ˜¯åœ¨å·ç§¯ç¥ç»ç½‘ç»œä¸­ã€‚

**å‚æ•°**ï¼š

-   `in_channels`: è¾“å…¥æ•°æ®çš„é€šé“æ•°ã€‚ä¾‹å¦‚ï¼Œå¯¹äºå½©è‰²å›¾åƒï¼Œ`in_channels` ä¸º3ï¼ˆåˆ†åˆ«å¯¹åº”çº¢ã€ç»¿ã€è“ä¸‰ç§é¢œè‰²ï¼‰ï¼›å¯¹äºç°åº¦å›¾åƒï¼Œ`in_channels` ä¸º1ã€‚
-   `out_channels`: å·ç§¯äº§ç”Ÿçš„ç‰¹å¾å›¾ï¼ˆFeature Mapsï¼‰çš„æ•°é‡ã€‚è¿™ä¸ªå‚æ•°å¯ä»¥è¢«è§†ä¸ºå­¦ä¹ ç‰¹å¾çš„æ•°é‡ã€‚
-   `kernel_size`: å·ç§¯æ ¸çš„å°ºå¯¸ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ•´æ•°æˆ–ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•´æ•°çš„å…ƒç»„ï¼ˆåˆ†åˆ«è¡¨ç¤ºé«˜å’Œå®½ï¼‰ã€‚å·ç§¯æ ¸æ˜¯ç”¨äºæ‰«æå›¾åƒçš„æ»‘åŠ¨çª—å£ã€‚
-   `stride`: å·ç§¯æ ¸ç§»åŠ¨çš„æ­¥é•¿ï¼Œå¯ä»¥æ˜¯å•ä¸ªæ•´æ•°æˆ–ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•´æ•°çš„å…ƒç»„ã€‚æ­¥é•¿æ§åˆ¶äº†å·ç§¯æ ¸åœ¨è¾“å…¥å›¾åƒä¸Šæ»‘åŠ¨çš„é€Ÿåº¦ã€‚
-   `padding`: åœ¨è¾“å…¥æ•°æ®çš„å‘¨å›´æ·»åŠ çš„é›¶çš„å±‚æ•°ã€‚å¯ä»¥æ˜¯å•ä¸ªæ•´æ•°æˆ–ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªæ•´æ•°çš„å…ƒç»„ã€‚è¡¥é›¶å¯ä»¥ä¿è¯è¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ä¸å˜æˆ–è€…æ§åˆ¶è¾“å‡ºç‰¹å¾å›¾çš„å°ºå¯¸ã€‚
-   `dilation`: å·ç§¯æ ¸å…ƒç´ ä¹‹é—´çš„é—´è·ã€‚å®ƒå¯ä»¥è¢«ç”¨äºæ§åˆ¶å·ç§¯æ ¸è¦†ç›–çš„ç©ºé—´å°ºå¯¸ï¼Œè€Œä¸æ”¹å˜å·ç§¯æ ¸ä¸­çš„å…ƒç´ æ•°é‡ã€‚
-   `groups`: æ§åˆ¶è¾“å…¥å’Œè¾“å‡ºä¹‹é—´çš„è¿æ¥ã€‚é»˜è®¤æƒ…å†µä¸‹ï¼Œgroups=1ï¼Œæ„å‘³ç€æ¯ä¸ªè¾“å…¥é€šé“ä¸æ¯ä¸ªè¾“å‡ºé€šé“éƒ½è¿æ¥ã€‚å¦‚æœgroups=2ï¼Œé‚£ä¹ˆå‰ä¸€åŠçš„è¾“å…¥é€šé“ä¸å‰ä¸€åŠçš„è¾“å‡ºé€šé“è¿æ¥ï¼Œåä¸€åŠåŒç†ã€‚å½“`groups=in_channels`å’Œ`out_channels`æ—¶ï¼Œå·ç§¯æ“ä½œå°±å˜æˆäº†æ·±åº¦å·ç§¯ã€‚
-   `bias`: å¦‚æœè®¾ç½®ä¸ºTrueï¼Œé‚£ä¹ˆå‘å·ç§¯æ·»åŠ åç½®ã€‚é»˜è®¤ä¸ºTrueã€‚

```python
class My_Model(nn.Module):
    def __init__(self):
        super().__init__()
        # torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)
        # torch.nn.MaxPool2d(kernel_size, stride, padding)
        # input_size: [3, 128, 128]
        self.cnn_pool = nn.Sequential(  # CNN
            nn.Conv2d(3, 64, 3, 1, 1),  # output_size: [64, 128, 128]
            nn.BatchNorm2d(64),  
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # Pooling to: [64, 64, 64]

            nn.Conv2d(64, 128, 3, 1, 1),  # output_size: [128, 64, 64]
            nn.BatchNorm2d(128),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # Pooling to: [128, 32, 32]

            nn.Conv2d(128, 256, 3, 1, 1),  # output_size: [256, 32, 32]
            nn.BatchNorm2d(256),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # Pooling to: [256, 16, 16]

            nn.Conv2d(256, 512, 3, 1, 1),  # output_size: [512, 16, 16]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # Pooling to: [512, 8, 8]

            nn.Conv2d(512, 512, 3, 1, 1),  # output_size: [512, 8, 8]
            nn.BatchNorm2d(512),
            nn.ReLU(),
            nn.MaxPool2d(2, 2, 0),  # Pooling to: [512, 4, 4]
        ) 
        self.fullc = nn.Sequential(  # Fully-connected NN
            nn.Linear(512 * 4 * 4, 1024),
            nn.ReLU(),
            nn.Linear(1024, 512),
            nn.ReLU(),
            nn.Linear(512, 11)
        )

    def forward(self, x):
        out = self.cnn_pool(x)
        out = out.view(out.size()[0], -1)  # å±•å¹³æ“ä½œï¼ˆFlattenï¼‰ï¼šå¸¸å¸¸åœ¨å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ä¹‹é—´è¿›è¡Œï¼Œå› ä¸ºå…¨è¿æ¥å±‚éœ€è¦çš„è¾“å…¥æ˜¯ä¸€ç»´çš„
        return self.fullc(out)
```

****

### RNN (Recurrent Neural Network)

**==å·²ç»å¯ä»¥è¢«Self-attentionå–ä»£ï¼ï¼ï¼è§ <u>Self-attention in [Modules](./Modules.md)</u>==**

**[PARAMETERS of nn.RNN](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html?highlight=rnn#torch.nn.RNN)**  

#### Application

<img src="images/image-20230909115712333.png" alt="image-20230909115712333" style="zoom: 33%;" /><img src="images/image-20230909115825005.png" alt="image-20230909115825005" style="zoom: 33%;" />

<img src="images/image-20230909120120367.png" alt="image-20230909120120367" style="zoom: 33%;" /><img src="images/image-20230909115932081.png" alt="image-20230909115932081" style="zoom: 33%;" />

**å¾ªç¯ç¥ç»ç½‘ç»œï¼ˆRecurrent Neural Networkï¼ŒRNNï¼‰**æ˜¯ä¸€ç§ä¸“é—¨å¤„ç†**åºåˆ—æ•°æ®**ï¼ˆä¾‹å¦‚ï¼Œæ—¶é—´åºåˆ—æ•°æ®\<éŸ³é¢‘>æˆ–æ–‡æœ¬ï¼‰çš„ç¥ç»ç½‘ç»œï¼ˆå¯ä»¥å¯¹è¯­éŸ³åˆ†ç±»ï¼Œä¸ç”¨ä¸€èˆ¬çš„åˆ†ç±»æ¨¡å‹ï¼‰ã€‚RNNä¸æ™®é€šçš„å…¨è¿æ¥ç¥ç»ç½‘ç»œå’Œå·ç§¯ç¥ç»ç½‘ç»œä¸åŒï¼Œå®ƒèƒ½å¤Ÿ**å¤„ç†åºåˆ—é•¿åº¦å¯å˜çš„æ•°æ®**ï¼Œ<u>åœ¨å¤„ç†æ¯ä¸ªå…ƒç´ æ—¶ï¼Œå®ƒéƒ½ä¼šè®°ä½å‰é¢å…ƒç´ çš„ä¿¡æ¯</u>ã€‚

**è®­ç»ƒæ—¶æ‰€æœ‰è¾“å…¥éƒ½ç»™å‡ºï¼Œå¹¶è¡Œè¾“å…¥ï¼š**

<img src="images/image-20230909141309461.png" alt="image-20230909141309461" style="zoom:50%;" />

**æµ‹è¯•æ—¶åªæä¾›ç¬¬ä¸€æ­¥è¾“å…¥ï¼Œåœ¨ä¸Šä¸€æ­¥è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­æŠ½æ ·ä½œä¸ºå…¶çŠ¶æ€çš„æœ€ç»ˆè¾“å‡ºï¼Œå¹¶ä½œä¸ºä¸‹ä¸€æ­¥çš„è¾“å…¥ï¼ˆæ”¹ä¸ºone-hotï¼‰ï¼Œä¸²è¡Œè¾“å…¥ï¼š**

<img src="images/image-20230909141728529.png" alt="image-20230909141728529" style="zoom:50%;" />

***æ­¤å¤–ä¹Ÿå¯ä»¥æŠŠéåºåˆ—æ•°æ®æ‹†è§£æˆåºåˆ—ä¿¡æ¯å­¦ä¹ ï¼Œè¿›è€Œä¼˜åŒ–å­¦ä¹ æ•ˆæœï¼š***

<img src="images/image-20230909120400575.png" alt="image-20230909120400575" style="zoom:50%;" />

<img src="images/image-20230909120458610.png" alt="image-20230909120458610" style="zoom:50%;" />

#### Layers & Structure

RNNçš„åŸºæœ¬æ€æƒ³æ˜¯åœ¨ç¥ç»ç½‘ç»œçš„**éšè—å±‚ä¹‹é—´å»ºç«‹å¾ªç¯è¿æ¥**ã€‚**æ¯ä¸€æ­¥éƒ½ä¼šæœ‰ä¸¤ä¸ªè¾“å…¥ï¼šå½“å‰æ­¥çš„è¾“å…¥æ•°æ®å’Œä¸Šä¸€æ­¥çš„éšè—çŠ¶æ€ï¼ˆäºæ˜¯æ¯æ¬¡modelä¼šæœ‰ä¸¤ä¸ªè¾“å‡ºå€¼ï¼Œè®­ç»ƒè¿‡ç¨‹æ³¨æ„å·¦å€¼è®¾ä¸º`output, _`ï¼‰ã€‚**ç„¶åï¼Œè¿™ä¸¤ä¸ªè¾“å…¥ä¼šè¢«é€å…¥ç½‘ç»œï¼ˆé€šå¸¸æ˜¯ä¸€ä¸ªå…¨è¿æ¥å±‚æˆ–è€…ä¸€äº›æ›´å¤æ‚çš„ç»“æ„ï¼Œå¦‚LSTMæˆ–GRUå•å…ƒï¼‰ï¼Œç„¶åäº§ç”Ÿä¸€ä¸ªè¾“å‡ºå’Œæ–°çš„éšè—çŠ¶æ€ã€‚è¿™ä¸ªæ–°çš„éšè—çŠ¶æ€å°†è¢«ç”¨äºä¸‹ä¸€æ­¥çš„è®¡ç®—ã€‚

è¿™ä¸ªè¿‡ç¨‹å¯ä»¥å†™ä½œå¦‚ä¸‹å½¢å¼çš„**çŠ¶æ€è½¬ç§»æ–¹ç¨‹**ï¼š
$$
h_t = f(h_{t-1}, x_t) = h_{t-1}\cdot W_h + x_t\cdot W_x+bias
$$
å…¶ä¸­ï¼Œ**$h_t$ æ˜¯åœ¨æ—¶é—´tçš„éšè—çŠ¶æ€ï¼ˆHå…ƒå‘é‡ï¼‰**ï¼Œ$x_t$ æ˜¯åœ¨æ—¶é—´tçš„è¾“å…¥ï¼ˆDå…ƒå‘é‡ï¼‰ï¼Œ$f$ æ˜¯ä¸€ä¸ªçº¿æ€§å‡½æ•°ï¼Œå®ƒå®šä¹‰äº†<u>å¦‚ä½•ç”¨å‰ä¸€æ­¥çš„éšè—çŠ¶æ€å’Œå½“å‰çš„è¾“å…¥è®¡ç®—å¾—åˆ°å½“å‰çš„éšè—çŠ¶æ€</u>ï¼Œå…¶ä¸­çŠ¶æ€æƒé‡ $W_h$ ï¼ˆH*HçŸ©é˜µï¼‰ã€è¾“å…¥æƒé‡ $W_x$ ï¼ˆD*HçŸ©é˜µï¼‰å’Œåå·® $bias$ ï¼ˆHå…ƒå‘é‡ï¼‰åœ¨å„æ—¶åˆ»ï¼ˆtimestepï¼‰çš„**çŠ¶æ€è½¬ç§»æ–¹ç¨‹**ä¸­ä¿æŒä¸å˜ã€‚

<img src="images/image-20230909120618449.png" alt="image-20230909120618449" style="zoom:50%;" />

é€šå¸¸è¿˜éœ€è¦ä¸€å±‚**éçº¿æ€§æ¿€æ´»å‡½æ•°** â€”â€” $\tanh$ï¼š
$$
h_t = \tanh\ (h_{t-1}\cdot W_h + x_t\cdot W_x+bias)
$$
æ­¤æ—¶ä»…å¾—åˆ°äº†æ–°æ—¶åˆ»çš„çŠ¶æ€ï¼ˆé€šå¸¸å…ˆæ±‚å‡ºå‰å‘ä¼ æ’­æ±‚å‡ºæ‰€æœ‰çŠ¶æ€å­˜åœ¨æ•°ç»„ï¼Œå†å¦å¤–è®¡ç®—æ‰€æœ‰è¾“å‡ºï¼‰ï¼Œ**å„çŠ¶æ€ä¸‹ï¼ˆæ—¶åˆ»ï¼‰çš„è¾“å‡º**è¿˜éœ€å¦å¤–æ ¹æ®è¯¥çŠ¶æ€ç”¨å¾—åˆ†æƒé‡ $W_y$ è®¡ç®—ï¼Œè¾“å‡ºå³ä¸åŒå¯èƒ½å–å€¼çš„å¯èƒ½æ€§å¾—åˆ†ï¼ˆæ­¤åè¿˜éœ€ç”¨ $softmax$ è½¬åŒ–ä¸ºæ¦‚ç‡åˆ†å¸ƒå€¼ï¼‰ï¼š
$$
y_t=W_y\cdot h_t
$$
***å„ç§ç»“æ„çš„RNNéƒ½æ»¡è¶³ä»¥ä¸Šå…¬å¼ï¼ŒåŒºåˆ«ä»…åœ¨äºæ¯ä¸€æ­¥çš„è¾“å…¥ $x$ æ˜¯å¦æœ‰ã€æ˜¯å¦æ¥è‡ªä¸Šæ¬¡è¾“å‡ºï¼š***

<img src="images/image-20230909140656609.png" alt="image-20230909140656609" style="zoom:50%;" />

<img src="images/image-20230909140633758.png" alt="image-20230909140633758" style="zoom:50%;" />

<img src="images/image-20230909140546263.png" alt="image-20230909140546263" style="zoom:50%;" />

<img src="images/image-20230909141012278.png" alt="image-20230909141012278" style="zoom:50%;" />

**$f_W$æ˜¯å¾ªç¯ä½¿ç”¨çš„ï¼ï¼ï¼ï¼æ‰€ä»¥ç§°ä¸ºå¾ªç¯ç¥ç»ç½‘ç»œ**

#### Backprop

<img src="images/image-20230909142544735.png" alt="image-20230909142544735" style="zoom:50%;" />

<img src="images/image-20230909142650615.png" alt="image-20230909142650615" style="zoom:50%;" />

#### Problems

<img src="images/image-20230909164318037.png" alt="image-20230909164318037" style="zoom:50%;" />

å¤„ç†é•¿åºåˆ—æ—¶ï¼Œå®ƒä»¬å¾€å¾€ä¼šé‡åˆ°æ‰€è°“çš„æ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸é—®é¢˜ã€‚è¿™äº›é—®é¢˜å·²ç»æœ‰ä¸€äº›è§£å†³æ–¹æ¡ˆï¼Œä¾‹å¦‚**é•¿çŸ­æœŸè®°å¿†ï¼ˆLong Short-Term Memoryï¼ŒLSTMï¼‰ç½‘ç»œ**å’Œ**é—¨æ§å¾ªç¯å•å…ƒï¼ˆGated Recurrent Unitï¼ŒGRUï¼‰**ã€‚

```python
class My_Model(nn.Module):
    def __init__(self, input_size, hidden_size):
        super().__init__()
        self.layers = nn.RNN(input_size, hidden_size, batch_first=True) 
        # or "nn.LSTM"/ "nn.GRU"

    def forward(self, x):  # 2 outputs !!!!!
        # x => (batch_size, sequence_length, input_size)
        # hidden => (num_layers, batch_size, hidden_size)
        x, hidden = self.layers(x)
        # (LSTM: x, (hidden, cell_state) = ...)
        return x, hidden
```

å¦‚æœ`batch_first=True`ï¼Œé‚£ä¹ˆè¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥ä¸º`(batch, seq, feature)`ï¼Œå³æ‰¹é‡å¤§å°ï¼ˆbatch sizeï¼‰æ˜¯å¼ é‡çš„ç¬¬ä¸€ç»´åº¦ã€‚å¦‚æœ`batch_first=False`ï¼ˆè¿™æ˜¯é»˜è®¤è®¾ç½®ï¼‰ï¼Œé‚£ä¹ˆè¾“å…¥å’Œè¾“å‡ºå¼ é‡çš„å½¢çŠ¶åº”è¯¥ä¸º`(seq, batch, feature)`ï¼Œå³åºåˆ—é•¿åº¦ï¼ˆsequence lengthï¼‰æ˜¯å¼ é‡çš„ç¬¬ä¸€ç»´åº¦ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ`batch_first=True`å¯èƒ½æ›´ç›´è§‚ï¼Œå› ä¸ºæˆ‘ä»¬é€šå¸¸ä¼šå°†æ‰¹é‡å¤§å°ä½œä¸ºç¬¬ä¸€ç»´åº¦ã€‚ä½†æœ‰æ—¶ï¼Œå‡ºäºæ€§èƒ½ä¼˜åŒ–æˆ–ä¸ç‰¹å®šåº“çš„å…¼å®¹æ€§è€ƒè™‘ï¼Œå¯èƒ½ä¼šéœ€è¦`batch_first=False`ã€‚

****

### CNN & RNN

==**CNNå¸¸ç”¨ä½œå›¾åƒåˆ†ç±»ï¼ŒRNNå¸¸ç”¨ä½œæ–‡æœ¬ç”Ÿæˆï¼Œä¸¤è€…ç»“åˆå¯ä»¥åš Image Captioningï¼š**==

<img src="images/image-20230805110434586.png" alt="image-20230805110434586" style="zoom:50%;" />

<img src="images/image-20230909144016690.png" alt="image-20230909144016690" style="zoom:50%;" />

<img src="images/image-20230909144440206.png" alt="image-20230909144440206" style="zoom:50%;" />

<img src="images/image-20230909144520905.png" alt="image-20230909144520905" style="zoom:50%;" />

<img src="images/image-20230909144537625.png" alt="image-20230909144537625" style="zoom:50%;" />

****

### GNN (Graph Neural Networks)

å›¾ç¥ç»ç½‘ç»œï¼ˆGraph Neural Networksï¼Œç®€ç§°GNNï¼‰æ˜¯ä¸€ç§å¼ºå¤§çš„ç”¨äºå¤„ç†å›¾å½¢æ•°æ®çš„æ·±åº¦å­¦ä¹ æ¶æ„ã€‚å®ƒä»¬å·²ç»åœ¨å„ç§é¢†åŸŸï¼ˆåŒ…æ‹¬ç¤¾äº¤ç½‘ç»œåˆ†æã€æ¨èç³»ç»Ÿã€ç”Ÿç‰©ä¿¡æ¯å­¦ç­‰ï¼‰æ˜¾ç¤ºå‡ºä¼˜å¼‚çš„æ€§èƒ½ã€‚åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸï¼ŒGNNä¹Ÿæ‰¾åˆ°äº†ä¸€äº›æœ‰è¶£ä¸”æœ‰ç”¨çš„åº”ç”¨ã€‚

è™½ç„¶ä¸€èˆ¬çš„è®¡ç®—æœºè§†è§‰ä»»åŠ¡ï¼ˆä¾‹å¦‚å›¾åƒåˆ†ç±»æˆ–ç›®æ ‡æ£€æµ‹ï¼‰é€šå¸¸å¯ä»¥ç›´æ¥ä½¿ç”¨å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰æ¥å¤„ç†ï¼Œä½†æ˜¯ä¸€äº›æ›´å¤æ‚çš„é—®é¢˜ï¼ˆä¾‹å¦‚å¯¹åœºæ™¯ç†è§£çš„éœ€æ±‚ï¼ŒåŒ…æ‹¬å¯¹è±¡çš„å…³ç³»æˆ–ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼‰å¯èƒ½ä¼šå—ç›ŠäºGNNã€‚ä¾‹å¦‚ï¼Œä¸€å¼ å›¾ç‰‡å¯ä»¥è¢«çœ‹ä½œæ˜¯ä¸€ä¸ªå›¾ï¼Œå…¶ä¸­èŠ‚ç‚¹è¡¨ç¤ºç‰©ä½“ï¼Œè¾¹è¡¨ç¤ºç‰©ä½“é—´çš„å…³ç³»ï¼ŒGNNå°±å¯ä»¥ç”¨æ¥å¤„ç†è¿™æ ·çš„é—®é¢˜ã€‚

æ­¤å¤–ï¼ŒGNNåœ¨è§†é¢‘ç†è§£å’Œ3Dæ•°æ®å¤„ç†ä¸­ä¹Ÿæœ‰å¾ˆå¤šåº”ç”¨ï¼Œå› ä¸ºè¿™äº›æ•°æ®å½¢å¼çš„æ—¶é—´å’Œç©ºé—´å…³ç³»å¯ä»¥è‡ªç„¶åœ°è¢«å»ºæ¨¡ä¸ºå›¾ã€‚ä¾‹å¦‚ï¼Œåœ¨è§†é¢‘åˆ†æä¸­ï¼ŒGNNå¯ä»¥å¸®åŠ©æ•æ‰ä¸åŒå¸§ä¹‹é—´çš„å…³ç³»ï¼›åœ¨3Dç‚¹äº‘åˆ†æä¸­ï¼ŒGNNå¯ä»¥æ›´å¥½åœ°æ•æ‰ç©ºé—´ç‚¹ä¹‹é—´çš„å¤æ‚å…³ç³»ã€‚

æ€»çš„æ¥è¯´ï¼ŒGNNä¸ºè®¡ç®—æœºè§†è§‰æä¾›äº†ä¸€ç§å¼ºå¤§çš„å·¥å…·ï¼Œå°¤å…¶æ˜¯åœ¨éœ€è¦ç†è§£å¯¹è±¡é—´å¤æ‚å…³ç³»å’Œä¸Šä¸‹æ–‡ä¿¡æ¯çš„åœºæ™¯ä¸­ã€‚ç„¶è€Œï¼Œå®ƒå¹¶éé€‚åˆæ‰€æœ‰çš„è§†è§‰ä»»åŠ¡ï¼Œè€Œæ˜¯ä½œä¸ºä¸€ä¸ªè¡¥å……å·¥å…·ï¼Œå’ŒCNNç­‰å…¶ä»–è§†è§‰å¤„ç†æ¨¡å‹å…±åŒä½¿ç”¨ã€‚

****



## 2 Modes of Model (nn.Module) & ` with torch.no_grad()`:

****

|                                                              |     è®­ç»ƒæ—¶`.train()`      |            éªŒè¯/ æµ‹è¯•æ—¶`.eval()`             |
| :----------------------------------------------------------: | :-----------------------: | :------------------------------------------: |
| æ·»åŠ **`with torch.no_grad():`**ç¦æ­¢åœ¨`.forward()`æ—¶å­˜å‚¨åå‘ä¼ æ’­è¦ç”¨çš„å€¼ï¼ˆ`.forward()`é»˜è®¤å†…ç½®è¿™ä¸€æ“ä½œï¼‰ |             âŒ             |       âœ”ï¸ï¼ˆèŠ‚çœå­˜å‚¨ä¸”åŠ é€Ÿ`.forward()`ï¼‰        |
|      è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼ˆå¦‚æœå¯ä»¥ï¼‰ã€**`.eval()`**å·²ä¿è¯ã€‘       |             âœ”ï¸             |                      âŒ                       |
| æ®æ¦‚ç‡ï¼ˆç»™å®šå‚æ•°ï¼‰éšæœºå…³é—­ç¥ç»å…ƒï¼ˆdropoutï¼‰ã€**`.eval()`**å·²ä¿è¯ã€‘ |             âœ”ï¸             |                      âŒ                       |
|  `.BatchNorm2d`ï¼ˆå¦‚æœæ¨¡å‹ä¸­è®¾ç½®äº†ï¼‰ã€**`.eval()`**å·²ä¿è¯ã€‘   | è®¡ç®—æ¯ä¸ªbatchçš„å‡å€¼å’Œæ–¹å·® | ä½¿ç”¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è®¡ç®—å¾—åˆ°çš„ç§»åŠ¨å¹³å‡å‡å€¼å’Œæ–¹å·® |

