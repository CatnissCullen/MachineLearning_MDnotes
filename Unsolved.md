# Unsolved Problems

****



## Position Encoding in Self-Attention

How to generate the positional vectors?

cutting-edge paper: https://arxiv.org/abs/2003.09229



## Encoder in Transformer

Best structure?

cutting-edge paper: 

-   On Layer Normalization in the Transformer Architecture - https://arxiv.org/abs/2002.04745
-   PowerNorm: Rethinking Batch Normalization in Transformers - https://arxiv.org/abs/2003.07845



## BLEU of Transformer

How to optimize as Loss Function during Training without RL?

with RL: https://arxiv.org/abs/1511.06732



