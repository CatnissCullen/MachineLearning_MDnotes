# DDPM 理解记录

****



（个人当前理解）

## 为什么要扩散？

**问题：要优化 Score ，但是数据密度低的地方 Score 估计不准，并且这些地方占比大，导致前进方向脱轨，目标分布采样不准** 

**解决：给目标分布进行多次加噪，噪声从强到弱（强噪声使得更大区域的梯度变显著易估计，弱噪声使得各处梯度更贴合原目标分布），使得目标分布分次膨胀，每膨胀一次，就有一部分原先密度低的地方变得更密集，梯度更显著，此时再实时学习这里的梯度就会更准确；最终当噪声强度降到 0 ，而膨胀范围覆盖整个目标分布，即目标分布扩散成纯高斯噪声时，就能学习完原目标分布内各处的梯度**



## 为什么要用分时间步的公式训练而不是用一步到位的公式？

训练一定是有误差的。由于对扩散的定义是马尔可夫链，因此有条件通过独立训练各时间步来独立减小各分步误差，从而减小总误差。如果一步到位产生的是累积误差，便很难减小。例如原始的 VAE ，编码器和解码器都是整体神经网络，编码一步到位，解码再一步到位，生成结果比 DDPM 模糊很多。



## 多样性

-   **输入的batch中各样本间特征越多样，输出的中间Interpolation就越清晰越多样**

    输入相当于告诉模型这个真实的分布中存在的特征模式，输入的batch特征越多变模型对真实分布的探索就越广，认为可选的特征模式就越多，能生成的特征模式的选择就更多

    （待实验：小 batch 和大 batch ）

-   **输入的时间步范围越大，最终加噪结果就越接近纯噪声，输出的中间Interpolation就越清晰越多样**

    每个时间步的加噪相当于在图层树上从原始叶子层（目标分布）往上添加一层（最高层只有树根），高层子根越高，反向过程能在叶子层生出的新叶子就更多

    （论文已实验：小 t 范围和大 t 范围）

    ![image-20240710165038916](./img/image-20240710165038916.png)



## 阅读 Stable Diffusion 后回顾 DDPM 的疑问

论文指明，扩散晚期（大概 t=500 以后）的加噪结果捕捉的是语义特征，从扩散晚期开始采样生成结果的图像含义区别更显著；扩散早期（大概 t=500 以前）的加噪结果捕捉的是感知细节特征，从扩散早期开始采样生成结果只有细节差异。这与对扩散时间步总长越大生成结果更多样的解释有区别，不知是两种原因都有还是某一种更合理？