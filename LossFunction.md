# Loss Function 

***to evaluate GOODNESS OF MODEL!!!! (the ability of generalization)***



## MSE Loss

它通常用于回归问题。在回归问题中，我们试图预测一个连续的数值，比如预测房价、股票价格等。MSE会计算模型预测值与真实值之间的平均平方差，衡量预测值与真实值之间的偏差。MSE的值越小，说明模型的预测效果越好。

```python
Loss = pytorch.nn.MSELoss()
```



## Cross Entropy Loss

它通常用于分类问题。在分类问题中，我们试图预测一个离散的类别标签，如图像分类、文本分类等。交叉熵损失衡量的是模型预测的概率分布与真实的概率分布之间的距离，是衡量模型预测概率与真实情况一致性的一个度量。交叉熵损失值越小，说明模型的预测效果越好。

在多分类问题中，我们常常使用**softmax**函数将网络的输出**转换为概率分布**。这个预估的概率分布Q与数据的真实分布P（通常是one-hot编码，只有真实类别的位置为1，其余为0）之间的差异，就可以用交叉熵损失来度量：
$$
L = - Σ P(x) log Q(x)
$$
其中，∑表示对所有可能的事件求和，p(x)是真实分布下事件x发生的概率，q(x)是预测分布下事件x发生的概率。

```python
Loss = pytorch.nn.CrossEntropy()  # 已经包含Softmax（自动将一层Softmax加到模型输出后）！！！！！！！
```

这就是交叉熵损失。你可以看到，它在形式上就是P和Q的交叉熵。如果网络的预测很准确（也就是说，Q接近于P），那么交叉熵损失就会很小。反之，如果网络的预测偏离了真实情况（也就是说，Q远离了P），那么交叉熵损失就会变大。

在多分类问题中，目标类别通常是用"one-hot"编码来表示的，即在真实类别对应的位置上为1，其他位置为0。这样的编码方式，我们称为P。

下面用一个形象的例子来解释这个公式。

假设你在进行一个图片分类任务，每张图片都应被分为猫、狗、或鸟其中的一种。对于一张特定的图片，它真实的标签可能是"猫"，那么真实分布P就是 [1, 0, 0]。也就是说，这张图片是猫的概率是1，是狗或鸟的概率都是0。

然后，你的模型可能预测这张图片为猫、狗、鸟的概率分别是0.7、0.2、0.1，那么你的预测分布Q就是 [0.7, 0.2, 0.1]。

那么，对于这张图片，交叉熵就是：`Cross-Entropy = - (1 * log(0.7) + 0 * log(0.2) + 0 * log(0.1))`。可以看到，如果你的预测分布完全正确，那么交叉熵就为0，因为`-log(1) = 0`；但如果你的预测分布完全错误，比如你预测这张图片是猫的概率为0，那么交叉熵就会变为无穷大，因为`-log(0)`为无穷大。



## Use Maximum Likelihood instead

**似然函数（Likelihood）**在机器学习中的含义：建立的模型（所假设的分布）能生成（generate，即sample\<`v.`>，抽样出）训练集的可能性（likelihood），即训练集属于该假设的分布的可能性、训练集真实所属分布与该假设的分布的相似度**（事实上似然函数取对数后的相反数就是 Cross Entropy！！！）**。

![image-20230723114252239](images/image-20230723114252239.png)

对分类问题采用概率模型，便可以使用连续的**似然函数**的值作为模型评估对象，通过优化预估分布函数和其参数的估计公式（关于训练集的统计量）来使似然函数最大即可实现对模型本身的优化。



## To Prevent Overfitting: L2 Regularization

L2正则化是一种常用的防止过拟合的技术，也被称为权重衰减。它的基本思想是通过在损失函数中添加一个额外的项来惩罚大的权重值，从而防止模型过于复杂。这种额外的项是模型权重的平方和的一部分，因此称为"L2"正则化。

具体来说，如果没有正则化，我们的目标可能只是最小化损失函数（例如，均方误差）：

$$
L = Σ(y - f(x))^2
$$
在这里，y是真实的目标值，f(x)是模型的预测值。

当我们加入L2正则化时，我们的目标变为最小化损失函数和正则化项之和：

$$
L = Σ(y - f(x))^2 + λΣw^2
$$
在这里，w代表模型的权重，λ是一个超参数，控制正则化的强度。请注意，正则化项Σw^2是所有权重平方和。

通过这种方式，L2正则化鼓励模型使用较小的权重。这通常会导致模型的复杂度降低，使得模型对于训练数据的小的变化不那么敏感，从而提高了模型的泛化能力。