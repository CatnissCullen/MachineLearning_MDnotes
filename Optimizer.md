# Optimizer



## Least Square

这种方法是直接解析的，也就是说，它可以直接找到最小化损失函数（通常是均方误差）的参数值。在线性回归中，最小二乘法的解可以直接通过数学公式计算出来，这使得计算速度很快。然而，最小二乘法需要计算输入数据的协方差矩阵的逆矩阵，如果输入数据的维度很高，或者协方差矩阵是奇异矩阵（没有逆矩阵），那么最小二乘法就不能使用。此外，最小二乘法也不能直接应用于非线性模型或者有正则化的模型。

## Gradient Descent

这种方法是迭代的，它通过多次更新参数来逐渐最小化损失函数。在每次迭代中，它都会计算损失函数的梯度，然后沿着梯度的反方向更新参数。梯度下降法的优点是它可以应用于任何可微的损失函数和任何模型，包括线性模型、非线性模型、有正则化的模型等。而且，梯度下降法不需要计算协方差矩阵的逆矩阵，因此可以应用于高维数据。然而，梯度下降法的缺点是需要手动设置学习率参数，且收敛速度可能较慢，尤其在损失函数的形状不理想（如有很多局部最小值）时。

## Stochastic Gradient Descent

在标准的梯度下降法中，我们会计算整个数据集的平均梯度来更新参数。这需要对整个数据集进行一次完整的遍历，计算代价可能非常大。

而在SGD中，每次迭代只随机选择一个（或一小部分）样本来计算梯度并更新参数。这样可以大大减少计算量，使得算法能够更快地进行。但由于每次只使用部分数据，导致更新方向可能会有一些噪声，收敛路径可能会“摇摆”。

因此，SGD在处理大规模数据集时，相对于传统的梯度下降法有明显优势。但需要仔细选择学习率和其他超参数，以保证算法的稳定性和收敛速度。

## Adam

<u>***（also see goodnotes）***</u>

Adam（Adaptive Moment Estimation）是一种常用的深度学习优化算法，由Diederik P. Kingma和Jimmy Lei Ba在2015年的论文"Adam: A Method for Stochastic Optimization"中提出。

Adam结合了两种其他的优化算法的优点：RMSProp（Root Mean Square Propagation）和Momentum。RMSProp算法为每个参数保持一个单独的学习率，该学习率是根据参数的最近梯度的平均值来调整的；而Momentum算法则是在更新参数时考虑上一次梯度的方向，从而加速学习过程并降低震荡。

Adam算法使用"一阶矩（梯度的均值）"和"二阶矩（梯度的未中心化方差）"的估计来自适应地调整每个参数的学习率。因此，Adam优化器能够适应复杂、非平稳和/或稀疏的优化问题，而**无需人为调整学习率**，对于深度学习任务特别有效。

Adam优化器和梯度下降（包括批量梯度下降，随机梯度下降，小批量梯度下降）都是用于优化模型参数的算法，但它们在实际应用中的效果和适用场景有所不同。

1.  **梯度下降**：

    **优点**：

    -   对于凸优化问题，梯度下降可以保证找到全局最优解。
    -   实现简单，容易理解。

    **缺点**：

    -   需要手动设定学习率，且对学习率非常敏感。太小的学习率可能导致收敛过慢，而太大的学习率可能导致震荡或者错过最优解。
    -   在处理非凸优化问题（如深度学习中的大部分问题）时，容易陷入局部最优解。
    -   对所有参数使用相同的学习率，无法自适应地调整不同参数的学习率。
    -   在处理稀疏数据或者梯度稀疏的问题时效果不佳。

2.  **Adam**：

    **优点**：

    -   结合了RMSProp和Momentum的优点，能够自适应地调整学习率，对学习率不敏感。
    -   对于非凸优化问题，能够找到相对较好的解。
    -   对于处理稀疏数据和梯度稀疏的问题效果较好。

    **缺点**：

    -   实现较复杂，计算量较大。
    -   尽管在实践中效果较好，但理论上并没有像梯度下降那样可以保证找到全局最优解的性质。

综上，当面对的问题较简单，或者你希望能够找到全局最优解时，可以考虑使用梯度下降。但在深度学习中，大多数优化问题都是非凸的，并且可能存在大量的参数和稀疏的梯度，此时使用Adam优化器通常会取得更好的效果。



## How to Escape Local Minima????????

### SGD has a BETTER CHANCE

### Momentum

### Start from Multiple Initials and Train Multiple times then pick the Finest

### Simulated Annealing

### Genetic Algorithms



## Q&A

**为什么要在每次训练的参数更新后马上用另一批数据验证？这样训练结果的损失值有什么用？**

在训练深度学习模型的过程中，经常需要对模型进行周期性的验证。验证的主要目的有以下几点：

1.  **模型性能监控**：通过观察模型在验证集上的性能，我们可以了解模型的学习情况，包括是否过拟合、欠拟合，以及是否在学习中产生了有效的特征表示等。在每个训练周期后进行验证，可以及时地获取这些信息。
2.  **早期停止（Early Stopping）**：当模型在验证集上的性能连续多个周期未见明显提升时，我们可能会选择提前结束训练，以节省计算资源和时间。这就是所谓的早期停止策略。
3.  **模型选择**：在每个训练周期后，我们都会在验证集上评估模型，根据模型的性能，可以保存当前最好的模型参数，这样即使在之后的训练中模型过拟合，我们仍然可以保留一个表现较好的模型。
4.  **超参数调整**：模型在验证集上的性能也可以作为超参数调整的依据。例如，如果模型在验证集上的损失值在逐渐增大，那么可能需要**调低学习率**或者**增大正则化项**等**（可以考虑一边训练-验证一边及时调整！！）**。

因此，虽然训练集的损失值是我们直接优化的目标，但是验证集的损失值在训练过程中也起着非常重要的作用，可以帮助我们了解模型的学习状况并进行相应的调整。

