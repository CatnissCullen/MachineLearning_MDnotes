# DDPM 通俗解释

****



初次学习时内心有很多认知方面的疑问，通过上网搜索都没有得到满意解释。在仔细学习数学推导的理论知识后对 DDPM 有了更透彻的认识，回顾之前自己的问题便能够有了自己的答案。不能说百分百准确但至少能自圆其说。



## 模型学习的究竟是什么？

建模时假设了加入的噪声是服从标准高斯分布的，而 U-net 学习的是在给定时间步 t 下，给定的加噪结果 `Xt` 到当前加入的噪声 $\epsilon^{(t)}$ 之间的映射。在模型假设下，扩散过程时各时间步抽样的噪声就是标准高斯分布中的样点。模型训练的目标，便是直接计算的某时间步下某次预测噪声和该次真实噪声之间的损失值。各时间步下，每个噪声点与加噪结果一一对应，从而可重建出各时间步下尽可能多的高斯噪声与加噪结果之间的直接联系，建立从加噪结果分布到噪声分布的映射（由于总训练次数远大于时间步总长，每个时间步下都能由多次训练抽样的噪声形成一个标准高斯分布中的子分布）。采样时输入纯噪声，便可以借预测的噪声用贝叶斯定理按图索骥找回之前的加噪结果。



## 为什么要扩散？

扩散就是对原图像逐步加噪，其实是对原图像进行高层特征到低层特征的分解，从而如果学习了噪声所在分布，就进而学习到了各层特征快照所在的分布。所以如果扩散时间步越多，模型就可以在更多层特征的分布下进行学习，扩充各层特征分布的真实数据点，这样最终各层特征叠加后能够组成的原图像就会越具有多样性。这是和 VAE 相比，从更好的方法上解决过拟合问题。VAE 强行让生成的图片以标准正态分布为目标来生成，以计算概率分布间相似度（KL散度）的方法解决 AE 一对一死记性学习的过拟合问题，但编码和解码都是黑盒的模型，人为约束条件还是太少，结果模糊；而 DDPM 是假设了确定的扩散和去噪公式（白盒），只用模型学习从标准正太分布抽样的噪声，不仅各时间步下图像编码解码公式的确定性使结果清晰，而且可以通过增多时间步减少过拟合。



## 为什么要用分时间步的公式训练而不是用一步到位的公式？

训练一定是有误差的。由于对扩散的定义是马尔可夫链，因此有条件通过独立训练各时间步来独立减小各分步误差，从而减小总误差。如果一步到位产生的是累积误差，便很难减小。例如原始的 VAE ，编码器和解码器都是整体神经网络，编码一步到位，解码再一步到位，生成结果比 DDPM 模糊很多。



## 阅读 Stable Diffusion 后回顾 DDPM 的疑问

论文指明，扩散晚期（大概 t=500 以后）的加噪结果捕捉的是语义特征，从扩散晚期开始采样生成结果的图像含义区别更显著；扩散早期（大概 t=500 以前）的加噪结果捕捉的是感知细节特征，从扩散早期开始采样生成结果只有细节差异。这与我对扩散时间步总长越大生成结果更多样的解释有区别，不知是两种原因都有还是某一种更合理？